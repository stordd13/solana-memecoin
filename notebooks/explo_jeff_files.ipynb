{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d3c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bc82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = Path(\n",
    "        \"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/data_onchain_merged_tokens_list.parquet\"\n",
    "    )\n",
    "\n",
    "lf_raw = pl.scan_parquet(\n",
    "        data_file_path,\n",
    "        low_memory=False,\n",
    "        cache=False,\n",
    "        use_statistics=True,\n",
    "        parallel=\"auto\",\n",
    "        rechunk=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee2b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (66_494_523, 36)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_utc</th><th>price</th><th>token_address</th><th>token_name</th><th>token_symbol</th><th>bonding_curve_address</th><th>mint_authority</th><th>signature</th><th>decimals</th><th>metadata_uri</th><th>initial_supply</th><th>creator</th><th>creators</th><th>is_mutable</th><th>virtual_token_reserves</th><th>virtual_sol_reserves</th><th>real_token_reserves</th><th>virtual_token_reserves.buy</th><th>virtual_sol_reserves.buy</th><th>real_sol_reserves.buy</th><th>real_token_reserves.buy</th><th>seller_fee_basis</th><th>fee.buy</th><th>fee_recipient.buy</th><th>fee_basis_points.buy</th><th>slot</th><th>block_time</th><th>created_at</th><th>metadata_name</th><th>metadata_symbol</th><th>metadata_description</th><th>metadata_image</th><th>metadata_show_name</th><th>metadata_created_on</th><th>metadata_website</th><th>metadata_twitter</th></tr><tr><td>datetime[μs]</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>2025-05-17 23:50:00</td><td>3.8284e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:51:00</td><td>3.8284e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:52:00</td><td>4.0220e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:53:00</td><td>3.7744e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:54:00</td><td>3.7744e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2025-06-28 11:38:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Se…</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JA…</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6…</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreigg…</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySy…</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53E…</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8…</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiae…</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?…</td><td>null</td></tr><tr><td>2025-06-28 11:39:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Se…</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JA…</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6…</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreigg…</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySy…</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53E…</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8…</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiae…</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?…</td><td>null</td></tr><tr><td>2025-06-28 11:40:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Se…</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JA…</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6…</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreigg…</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySy…</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53E…</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8…</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiae…</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?…</td><td>null</td></tr><tr><td>2025-06-28 11:41:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Se…</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JA…</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6…</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreigg…</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySy…</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53E…</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8…</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiae…</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?…</td><td>null</td></tr><tr><td>2025-06-28 11:42:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Se…</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JA…</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6…</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreigg…</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySy…</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53E…</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8…</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this co…</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiae…</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?…</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (66_494_523, 36)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ date_utc  ┆ price     ┆ token_add ┆ token_nam ┆ … ┆ metadata_ ┆ metadata_ ┆ metadata_ ┆ metadata │\n",
       "│ ---       ┆ ---       ┆ ress      ┆ e         ┆   ┆ show_name ┆ created_o ┆ website   ┆ _twitter │\n",
       "│ datetime[ ┆ f64       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ n         ┆ ---       ┆ ---      │\n",
       "│ μs]       ┆           ┆ str       ┆ str       ┆   ┆ bool      ┆ ---       ┆ str       ┆ str      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ str       ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2025-05-1 ┆ 3.8284e-7 ┆ 1kMzbW3Jt ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
       "│ 7         ┆           ┆ fCQgXRjQh ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 23:50:00  ┆           ┆ bFkEDtAaZ ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ DYA…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-05-1 ┆ 3.8284e-7 ┆ 1kMzbW3Jt ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
       "│ 7         ┆           ┆ fCQgXRjQh ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 23:51:00  ┆           ┆ bFkEDtAaZ ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ DYA…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-05-1 ┆ 4.0220e-7 ┆ 1kMzbW3Jt ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
       "│ 7         ┆           ┆ fCQgXRjQh ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 23:52:00  ┆           ┆ bFkEDtAaZ ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ DYA…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-05-1 ┆ 3.7744e-7 ┆ 1kMzbW3Jt ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
       "│ 7         ┆           ┆ fCQgXRjQh ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 23:53:00  ┆           ┆ bFkEDtAaZ ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ DYA…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-05-1 ┆ 3.7744e-7 ┆ 1kMzbW3Jt ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
       "│ 7         ┆           ┆ fCQgXRjQh ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 23:54:00  ┆           ┆ bFkEDtAaZ ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ DYA…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 2025-06-2 ┆ 0.000004  ┆ zsAM6khYe ┆ you did   ┆ … ┆ true      ┆ https://p ┆ https://w ┆ null     │\n",
       "│ 8         ┆           ┆ 7UTjhDkKb ┆ not       ┆   ┆           ┆ ump.fun   ┆ ww.youtub ┆          │\n",
       "│ 11:38:00  ┆           ┆ JtPeQwicH ┆ search    ┆   ┆           ┆           ┆ e.com/wat ┆          │\n",
       "│           ┆           ┆ ZtH…      ┆ for this  ┆   ┆           ┆           ┆ ch?…      ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-06-2 ┆ 0.000004  ┆ zsAM6khYe ┆ you did   ┆ … ┆ true      ┆ https://p ┆ https://w ┆ null     │\n",
       "│ 8         ┆           ┆ 7UTjhDkKb ┆ not       ┆   ┆           ┆ ump.fun   ┆ ww.youtub ┆          │\n",
       "│ 11:39:00  ┆           ┆ JtPeQwicH ┆ search    ┆   ┆           ┆           ┆ e.com/wat ┆          │\n",
       "│           ┆           ┆ ZtH…      ┆ for this  ┆   ┆           ┆           ┆ ch?…      ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-06-2 ┆ 0.000004  ┆ zsAM6khYe ┆ you did   ┆ … ┆ true      ┆ https://p ┆ https://w ┆ null     │\n",
       "│ 8         ┆           ┆ 7UTjhDkKb ┆ not       ┆   ┆           ┆ ump.fun   ┆ ww.youtub ┆          │\n",
       "│ 11:40:00  ┆           ┆ JtPeQwicH ┆ search    ┆   ┆           ┆           ┆ e.com/wat ┆          │\n",
       "│           ┆           ┆ ZtH…      ┆ for this  ┆   ┆           ┆           ┆ ch?…      ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-06-2 ┆ 0.000004  ┆ zsAM6khYe ┆ you did   ┆ … ┆ true      ┆ https://p ┆ https://w ┆ null     │\n",
       "│ 8         ┆           ┆ 7UTjhDkKb ┆ not       ┆   ┆           ┆ ump.fun   ┆ ww.youtub ┆          │\n",
       "│ 11:41:00  ┆           ┆ JtPeQwicH ┆ search    ┆   ┆           ┆           ┆ e.com/wat ┆          │\n",
       "│           ┆           ┆ ZtH…      ┆ for this  ┆   ┆           ┆           ┆ ch?…      ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2025-06-2 ┆ 0.000004  ┆ zsAM6khYe ┆ you did   ┆ … ┆ true      ┆ https://p ┆ https://w ┆ null     │\n",
       "│ 8         ┆           ┆ 7UTjhDkKb ┆ not       ┆   ┆           ┆ ump.fun   ┆ ww.youtub ┆          │\n",
       "│ 11:42:00  ┆           ┆ JtPeQwicH ┆ search    ┆   ┆           ┆           ┆ e.com/wat ┆          │\n",
       "│           ┆           ┆ ZtH…      ┆ for this  ┆   ┆           ┆           ┆ ch?…      ┆          │\n",
       "│           ┆           ┆           ┆ co…       ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_raw.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38ffba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_21835/3122231484.py:1: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  lf_raw.columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['date_utc',\n",
       " 'price',\n",
       " 'token_address',\n",
       " 'token_name',\n",
       " 'token_symbol',\n",
       " 'bonding_curve_address',\n",
       " 'mint_authority',\n",
       " 'signature',\n",
       " 'decimals',\n",
       " 'metadata_uri',\n",
       " 'initial_supply',\n",
       " 'creator',\n",
       " 'creators',\n",
       " 'is_mutable',\n",
       " 'virtual_token_reserves',\n",
       " 'virtual_sol_reserves',\n",
       " 'real_token_reserves',\n",
       " 'virtual_token_reserves.buy',\n",
       " 'virtual_sol_reserves.buy',\n",
       " 'real_sol_reserves.buy',\n",
       " 'real_token_reserves.buy',\n",
       " 'seller_fee_basis',\n",
       " 'fee.buy',\n",
       " 'fee_recipient.buy',\n",
       " 'fee_basis_points.buy',\n",
       " 'slot',\n",
       " 'block_time',\n",
       " 'created_at',\n",
       " 'metadata_name',\n",
       " 'metadata_symbol',\n",
       " 'metadata_description',\n",
       " 'metadata_image',\n",
       " 'metadata_show_name',\n",
       " 'metadata_created_on',\n",
       " 'metadata_website',\n",
       " 'metadata_twitter']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b40650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/948930100.py:5: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    }
   ],
   "source": [
    "lf = lf_raw.select(\"token_address\", \"date_utc\", \"price\")\n",
    "tokens_address = (\n",
    "    lf.select(\"token_address\")\n",
    "    .unique()\n",
    "    .collect(streaming=True)\n",
    "    .get_column(\"token_address\")\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e9471ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (66_494_523, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token_address</th><th>date_utc</th><th>price</th></tr><tr><td>str</td><td>datetime[μs]</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>2025-05-17 23:50:00</td><td>3.8284e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>2025-05-17 23:51:00</td><td>3.8284e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>2025-05-17 23:52:00</td><td>4.0220e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>2025-05-17 23:53:00</td><td>3.7744e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA…</td><td>2025-05-17 23:54:00</td><td>3.7744e-7</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>2025-06-28 11:38:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>2025-06-28 11:39:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>2025-06-28 11:40:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>2025-06-28 11:41:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtH…</td><td>2025-06-28 11:42:00</td><td>0.000004</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (66_494_523, 3)\n",
       "┌─────────────────────────────────┬─────────────────────┬───────────┐\n",
       "│ token_address                   ┆ date_utc            ┆ price     │\n",
       "│ ---                             ┆ ---                 ┆ ---       │\n",
       "│ str                             ┆ datetime[μs]        ┆ f64       │\n",
       "╞═════════════════════════════════╪═════════════════════╪═══════════╡\n",
       "│ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA… ┆ 2025-05-17 23:50:00 ┆ 3.8284e-7 │\n",
       "│ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA… ┆ 2025-05-17 23:51:00 ┆ 3.8284e-7 │\n",
       "│ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA… ┆ 2025-05-17 23:52:00 ┆ 4.0220e-7 │\n",
       "│ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA… ┆ 2025-05-17 23:53:00 ┆ 3.7744e-7 │\n",
       "│ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYA… ┆ 2025-05-17 23:54:00 ┆ 3.7744e-7 │\n",
       "│ …                               ┆ …                   ┆ …         │\n",
       "│ zsAM6khYe7UTjhDkKbJtPeQwicHZtH… ┆ 2025-06-28 11:38:00 ┆ 0.000004  │\n",
       "│ zsAM6khYe7UTjhDkKbJtPeQwicHZtH… ┆ 2025-06-28 11:39:00 ┆ 0.000004  │\n",
       "│ zsAM6khYe7UTjhDkKbJtPeQwicHZtH… ┆ 2025-06-28 11:40:00 ┆ 0.000004  │\n",
       "│ zsAM6khYe7UTjhDkKbJtPeQwicHZtH… ┆ 2025-06-28 11:41:00 ┆ 0.000004  │\n",
       "│ zsAM6khYe7UTjhDkKbJtPeQwicHZtH… ┆ 2025-06-28 11:42:00 ┆ 0.000004  │\n",
       "└─────────────────────────────────┴─────────────────────┴───────────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2afb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "\n",
    "def compute_log_returns(df: pl.DataFrame) -> pl.Series | None:\n",
    "    \"\"\"Calcule les log-returns et retourne une Series Polars.\"\"\"\n",
    "    if df.height < 2:\n",
    "        return None\n",
    "    log_returns = (\n",
    "        df.select(\n",
    "            pl.col(\"price\")\n",
    "            .log().diff()\n",
    "            .alias(\"log_returns\")\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .get_column(\"log_returns\")\n",
    "    )\n",
    "    return log_returns if not log_returns.is_empty() else None\n",
    "\n",
    "def compute_volatility_log_return(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\"Calcule la volatilité (écart-type) des log-returns.\"\"\"\n",
    "    log_returns = compute_log_returns(df)\n",
    "    if log_returns is None:\n",
    "        return None\n",
    "    return float(log_returns.std())\n",
    "\n",
    "def compute_kurtosis_log_return(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\"Calcule le kurtosis des log-returns.\"\"\"\n",
    "    log_returns = compute_log_returns(df)\n",
    "    if log_returns is None:\n",
    "        return None\n",
    "    kurtosis_value = float(kurtosis(log_returns.to_numpy().flatten(), fisher=True, bias=False))\n",
    "    return max(1E-6, kurtosis_value)\n",
    "\n",
    "def compute_price_ratio(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\"Calcule le ratio prix max / prix min.\"\"\"\n",
    "    min_price, max_price = float(df[\"price\"].min()), float(df[\"price\"].max())\n",
    "    return min(1E6, max_price / (min_price + 1E-9))\n",
    "\n",
    "def compute_score(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\" Calcule un score normalisé entre 0 (mauvais) et 1 (excellent). \"\"\"\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "    df = df.sort(\"date_utc\")\n",
    "    price_ratio = min(100, compute_price_ratio(df))\n",
    "    if price_ratio == 1:\n",
    "        return 0.0\n",
    "    kurtosis_log_return = compute_kurtosis_log_return(df)\n",
    "    volatility_log_return = compute_volatility_log_return(df)\n",
    "\n",
    "    if kurtosis_log_return is None or volatility_log_return is None:\n",
    "        return None\n",
    "\n",
    "    score = (volatility_log_return / (kurtosis_log_return + 1E-9)) * (price_ratio - 1)\n",
    "    score = (np.exp(score) - 1) * (price_ratio - 1)\n",
    "    score = abs(score / (1 + score))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de880aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "507da7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_072_980, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token_address</th><th>date_utc</th><th>price</th></tr><tr><td>str</td><td>datetime[μs]</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:07:00</td><td>0.00085</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:08:00</td><td>0.001856</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:09:00</td><td>0.001977</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:10:00</td><td>0.002113</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:11:00</td><td>714.986041</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:02:00</td><td>0.000066</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:03:00</td><td>0.000066</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:04:00</td><td>0.000066</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:05:00</td><td>0.000067</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:06:00</td><td>0.000067</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_072_980, 3)\n",
       "┌─────────────────────────────────────────────┬─────────────────────┬────────────┐\n",
       "│ token_address                               ┆ date_utc            ┆ price      │\n",
       "│ ---                                         ┆ ---                 ┆ ---        │\n",
       "│ str                                         ┆ datetime[μs]        ┆ f64        │\n",
       "╞═════════════════════════════════════════════╪═════════════════════╪════════════╡\n",
       "│ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY ┆ 2025-05-15 06:07:00 ┆ 0.00085    │\n",
       "│ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY ┆ 2025-05-15 06:08:00 ┆ 0.001856   │\n",
       "│ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY ┆ 2025-05-15 06:09:00 ┆ 0.001977   │\n",
       "│ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY ┆ 2025-05-15 06:10:00 ┆ 0.002113   │\n",
       "│ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY ┆ 2025-05-15 06:11:00 ┆ 714.986041 │\n",
       "│ …                                           ┆ …                   ┆ …          │\n",
       "│ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL ┆ 2025-02-14 02:02:00 ┆ 0.000066   │\n",
       "│ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL ┆ 2025-02-14 02:03:00 ┆ 0.000066   │\n",
       "│ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL ┆ 2025-02-14 02:04:00 ┆ 0.000066   │\n",
       "│ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL ┆ 2025-02-14 02:05:00 ┆ 0.000067   │\n",
       "│ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL ┆ 2025-02-14 02:06:00 ┆ 0.000067   │\n",
       "└─────────────────────────────────────────────┴─────────────────────┴────────────┘"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/data_onchain_merged_tokens_list_high_score.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b019940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token_address</th><th>date_utc</th><th>price</th></tr><tr><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>5072980</td><td>5072980</td><td>5072980</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "┌───────────────┬──────────┬─────────┐\n",
       "│ token_address ┆ date_utc ┆ price   │\n",
       "│ ---           ┆ ---      ┆ ---     │\n",
       "│ u32           ┆ u32      ┆ u32     │\n",
       "╞═══════════════╪══════════╪═════════╡\n",
       "│ 5072980       ┆ 5072980  ┆ 5072980 │\n",
       "└───────────────┴──────────┴─────────┘"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "531472d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées : 5072980 lignes, 3478 tokens\n",
      "1. Ajout des features de base...\n",
      "2. Ajout des indicateurs techniques rapides...\n",
      "3. Ajout des features de détection pump/dump...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:50: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_mean(3, min_periods=1).over(\"token_address\").alias(\"ma_3\"),\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:51: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\").alias(\"ma_5\"),\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:52: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_mean(10, min_periods=5).over(\"token_address\").alias(\"ma_10\"),\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:55: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  (pl.col(\"price\") / pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\"))\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:59: DeprecationWarning: the argument `min_periods` for `Expr.rolling_std` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"log_returns\").rolling_std(5, min_periods=2).over(\"token_address\")\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:63: DeprecationWarning: the argument `min_periods` for `Expr.rolling_max` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  (pl.col(\"price\").rolling_max(5, min_periods=2).over(\"token_address\") /\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:64: DeprecationWarning: the argument `min_periods` for `Expr.rolling_min` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_min(5, min_periods=2).over(\"token_address\"))\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:78: DeprecationWarning: the argument `min_periods` for `Expr.rolling_sum` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  .rolling_sum(3, min_periods=1).over(\"token_address\")\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:83: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  .rolling_mean(3, min_periods=1).over(\"token_address\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Ajout des targets multi-step...\n",
      "✅ Feature engineering terminé: 45 colonnes\n",
      "\n",
      "💾 Features sauvegardées : /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/memecoin_features_complete.parquet\n",
      "\n",
      "Création des séquences...\n",
      "\n",
      "✅ Séquences créées :\n",
      "  - Nombre : 4933860\n",
      "  - Shape input : (4933860, 15, 10)\n",
      "  - Shape target : (4933860, 5)\n",
      "💾 Séquences sauvegardées : /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_raw.npz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "feature_engineering.py\n",
    "Feature engineering pour les données de memecoins\n",
    "\"\"\"\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class MemecoinsFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering spécialisé pour les memecoins\n",
    "    Transforme les données brutes (price, date_utc, token_address) \n",
    "    en features utilisables pour le ML\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df.sort([\"token_address\", \"date_utc\"])\n",
    "        \n",
    "    def add_core_features(self) -> pl.DataFrame:\n",
    "        \"\"\"Features essentielles calculables dès les premières minutes\"\"\"\n",
    "        self.df = self.df.with_columns([\n",
    "            # 1. Features temporelles basiques\n",
    "            pl.col(\"date_utc\").dt.minute().alias(\"minute_of_hour\"),\n",
    "            pl.col(\"date_utc\").dt.hour().alias(\"hour_of_day\"),\n",
    "            \n",
    "            # 2. Minutes depuis le lancement (crucial pour les memecoins)\n",
    "            (pl.col(\"date_utc\") - pl.col(\"date_utc\").first().over(\"token_address\"))\n",
    "            .dt.total_minutes()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(\"minutes_since_launch\"),\n",
    "            \n",
    "            # 3. Prix transformé (plus stable)\n",
    "            pl.col(\"price\").log().alias(\"log_price\"),\n",
    "            \n",
    "            # 4. Returns instantanés\n",
    "            pl.col(\"price\").pct_change().over(\"token_address\").fill_null(0).alias(\"returns\"),\n",
    "            pl.col(\"price\").log().diff().over(\"token_address\").fill_null(0).alias(\"log_returns\"),\n",
    "            \n",
    "            # 5. Prix relatif au prix initial\n",
    "            (pl.col(\"price\") / pl.col(\"price\").first().over(\"token_address\")).alias(\"price_multiple\"),\n",
    "        ])\n",
    "        return self.df\n",
    "    \n",
    "    def add_fast_indicators(self) -> pl.DataFrame:\n",
    "        \"\"\"Indicateurs calculables avec peu de points\"\"\"\n",
    "        self.df = self.df.with_columns([\n",
    "            # Rolling windows courts (3, 5, 10 minutes max)\n",
    "            pl.col(\"price\").rolling_mean(3, min_periods=1).over(\"token_address\").alias(\"ma_3\"),\n",
    "            pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\").alias(\"ma_5\"),\n",
    "            pl.col(\"price\").rolling_mean(10, min_periods=5).over(\"token_address\").alias(\"ma_10\"),\n",
    "            \n",
    "            # Écart au MA\n",
    "            (pl.col(\"price\") / pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\"))\n",
    "            .fill_null(1.0).alias(\"price_to_ma5_ratio\"),\n",
    "            \n",
    "            # Volatilité instantanée (rolling sur 5 minutes)\n",
    "            pl.col(\"log_returns\").rolling_std(5, min_periods=2).over(\"token_address\")\n",
    "            .fill_null(0).alias(\"volatility_5m\"),\n",
    "            \n",
    "            # Range de prix sur 5 minutes\n",
    "            (pl.col(\"price\").rolling_max(5, min_periods=2).over(\"token_address\") / \n",
    "             pl.col(\"price\").rolling_min(5, min_periods=2).over(\"token_address\"))\n",
    "            .fill_null(1.0).alias(\"price_range_5m\"),\n",
    "            \n",
    "            # Momentum simple\n",
    "            (pl.col(\"price\") / pl.col(\"price\").shift(5).over(\"token_address\"))\n",
    "            .fill_null(1.0).alias(\"momentum_5m\"),\n",
    "        ])\n",
    "        return self.df\n",
    "    \n",
    "    def add_pump_detection_features(self) -> pl.DataFrame:\n",
    "        \"\"\"Features spécifiques pour détecter les pumps/dumps\"\"\"\n",
    "        self.df = self.df.with_columns([\n",
    "            # Nombre de hausses consécutives\n",
    "            (pl.col(\"returns\") > 0).cast(pl.Int32)\n",
    "            .rolling_sum(3, min_periods=1).over(\"token_address\")\n",
    "            .alias(\"consecutive_ups_3m\"),\n",
    "            \n",
    "            # Vitesse de changement\n",
    "            pl.col(\"log_returns\").abs()\n",
    "            .rolling_mean(3, min_periods=1).over(\"token_address\")\n",
    "            .alias(\"avg_abs_change_3m\"),\n",
    "            \n",
    "            # Est-ce qu'on est en pump ? (>5% en 3 minutes)\n",
    "            (pl.col(\"price\") / pl.col(\"price\").shift(3).over(\"token_address\") > 1.05)\n",
    "            .fill_null(False).cast(pl.Int32)\n",
    "            .alias(\"is_pumping\"),\n",
    "            \n",
    "            # Est-ce qu'on est en dump ? (<-5% en 3 minutes)\n",
    "            (pl.col(\"price\") / pl.col(\"price\").shift(3).over(\"token_address\") < 0.95)\n",
    "            .fill_null(False).cast(pl.Int32)\n",
    "            .alias(\"is_dumping\"),\n",
    "        ])\n",
    "        return self.df\n",
    "    \n",
    "    def add_multi_step_targets(self, forecast_steps: int = 5) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute les targets pour prédire les N prochains points\n",
    "        forecast_steps: nombre de minutes à prédire dans le futur\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Pour chaque step futur\n",
    "        for step in range(1, forecast_steps + 1):\n",
    "            df = df.with_columns([\n",
    "                # Prix futur à chaque step\n",
    "                pl.col(\"price\").shift(-step).over(\"token_address\")\n",
    "                .alias(f\"target_price_t{step}\"),\n",
    "                \n",
    "                # Log prix (plus stable pour la prédiction)\n",
    "                pl.col(\"log_price\").shift(-step).over(\"token_address\")\n",
    "                .alias(f\"target_log_price_t{step}\"),\n",
    "                \n",
    "                # Return cumulé depuis maintenant jusqu'à ce step\n",
    "                ((pl.col(\"price\").shift(-step) / pl.col(\"price\")) - 1)\n",
    "                .over(\"token_address\")\n",
    "                .alias(f\"target_return_t{step}\"),\n",
    "                \n",
    "                # Direction binaire à chaque step\n",
    "                (pl.col(\"price\").shift(-step) > pl.col(\"price\"))\n",
    "                .over(\"token_address\")\n",
    "                .cast(pl.Int32)\n",
    "                .alias(f\"target_direction_t{step}\"),\n",
    "            ])\n",
    "        \n",
    "        # Ajouter des métriques agrégées sur la séquence future\n",
    "        df = df.with_columns([\n",
    "            # Prix max dans les N prochaines minutes\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_price_t{i}\") for i in range(1, forecast_steps + 1)\n",
    "            ]).list.max().alias(f\"target_max_price_next_{forecast_steps}m\"),\n",
    "            \n",
    "            # Prix min dans les N prochaines minutes\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_price_t{i}\") for i in range(1, forecast_steps + 1)\n",
    "            ]).list.min().alias(f\"target_min_price_next_{forecast_steps}m\"),\n",
    "            \n",
    "            # Y a-t-il un pump dans les N prochaines minutes? (>5%)\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_return_t{i}\") > 0.05 for i in range(1, forecast_steps + 1)\n",
    "            ]).list.any().cast(pl.Int32).alias(f\"target_has_pump_next_{forecast_steps}m\"),\n",
    "            \n",
    "            # Y a-t-il un dump dans les N prochaines minutes? (<-5%)\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_return_t{i}\") < -0.05 for i in range(1, forecast_steps + 1)\n",
    "            ]).list.any().cast(pl.Int32).alias(f\"target_has_dump_next_{forecast_steps}m\"),\n",
    "        ])\n",
    "        \n",
    "        self.df = df\n",
    "        return self.df\n",
    "    \n",
    "    def create_all_features(self, forecast_steps: int = 5) -> pl.DataFrame:\n",
    "        \"\"\"Pipeline complet de feature engineering\"\"\"\n",
    "        print(\"1. Ajout des features de base...\")\n",
    "        self.add_core_features()\n",
    "        \n",
    "        print(\"2. Ajout des indicateurs techniques rapides...\")\n",
    "        self.add_fast_indicators()\n",
    "        \n",
    "        print(\"3. Ajout des features de détection pump/dump...\")\n",
    "        self.add_pump_detection_features()\n",
    "        \n",
    "        print(\"4. Ajout des targets multi-step...\")\n",
    "        self.add_multi_step_targets(forecast_steps)\n",
    "        \n",
    "        print(f\"✅ Feature engineering terminé: {len(self.df.columns)} colonnes\")\n",
    "        return self.df\n",
    "\n",
    "\n",
    "def create_sequences_from_features(\n",
    "    df: pl.DataFrame,\n",
    "    sequence_length: int = 15,\n",
    "    forecast_steps: int = 5,\n",
    "    min_minutes_since_launch: int = 15\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Crée des séquences à partir du DataFrame avec features\n",
    "    Cette fonction est séparée de la classe pour plus de flexibilité\n",
    "    \"\"\"\n",
    "    \n",
    "    # Features pour l'input\n",
    "    feature_cols = [\n",
    "        \"minutes_since_launch\",\n",
    "        \"log_price\",\n",
    "        \"returns\",\n",
    "        \"price_multiple\",\n",
    "        \"price_to_ma5_ratio\",\n",
    "        \"volatility_5m\",\n",
    "        \"momentum_5m\",\n",
    "        \"is_pumping\",\n",
    "        \"is_dumping\",\n",
    "        \"avg_abs_change_3m\"\n",
    "    ]\n",
    "    \n",
    "    # Colonnes des targets\n",
    "    target_price_cols = [f\"target_log_price_t{i}\" for i in range(1, forecast_steps + 1)]\n",
    "    target_return_cols = [f\"target_return_t{i}\" for i in range(1, forecast_steps + 1)]\n",
    "    target_direction_cols = [f\"target_direction_t{i}\" for i in range(1, forecast_steps + 1)]\n",
    "    \n",
    "    # S'assurer qu'on a pas de NaN dans les colonnes critiques\n",
    "    check_cols = feature_cols + target_price_cols\n",
    "    df_clean = df.drop_nulls(subset=check_cols)\n",
    "    \n",
    "    sequences = []\n",
    "    target_sequences = []\n",
    "    target_returns = []\n",
    "    target_directions = []\n",
    "    metadata = []\n",
    "    \n",
    "    for token, group in df_clean.group_by(\"token_address\"):\n",
    "        group = group.sort(\"date_utc\")\n",
    "        \n",
    "        # Ne pas utiliser les toutes premières minutes\n",
    "        mask = group[\"minutes_since_launch\"] >= min_minutes_since_launch\n",
    "        group = group.filter(mask)\n",
    "        \n",
    "        if len(group) < sequence_length + forecast_steps:\n",
    "            continue\n",
    "        \n",
    "        # Extraire toutes les données nécessaires\n",
    "        features = group.select(feature_cols).to_numpy()\n",
    "        \n",
    "        # Targets : séquences futures complètes\n",
    "        future_prices = group.select(target_price_cols).to_numpy()\n",
    "        future_returns = group.select(target_return_cols).to_numpy()\n",
    "        future_directions = group.select(target_direction_cols).to_numpy()\n",
    "        \n",
    "        # Métriques agrégées\n",
    "        has_pump = group[f\"target_has_pump_next_{forecast_steps}m\"].to_numpy()\n",
    "        has_dump = group[f\"target_has_dump_next_{forecast_steps}m\"].to_numpy()\n",
    "        \n",
    "        # Créer des séquences\n",
    "        for i in range(sequence_length, len(features) - forecast_steps):\n",
    "            sequences.append(features[i-sequence_length:i])\n",
    "            target_sequences.append(future_prices[i])\n",
    "            target_returns.append(future_returns[i])\n",
    "            target_directions.append(future_directions[i])\n",
    "            \n",
    "            metadata.append({\n",
    "                \"token\": token[0],\n",
    "                \"timestamp\": group[\"date_utc\"][i],\n",
    "                \"minutes_since_launch\": int(group[\"minutes_since_launch\"][i]),\n",
    "                \"current_price\": float(group[\"price\"][i]),\n",
    "                \"has_pump_next\": int(has_pump[i]),\n",
    "                \"has_dump_next\": int(has_dump[i])\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"input_sequences\": np.array(sequences, dtype=np.float32),\n",
    "        \"target_sequences\": np.array(target_sequences, dtype=np.float32),\n",
    "        \"target_returns\": np.array(target_returns, dtype=np.float32),\n",
    "        \"target_directions\": np.array(target_directions, dtype=np.int32),\n",
    "        \"metadata\": metadata,\n",
    "        \"feature_names\": feature_cols,\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"forecast_steps\": forecast_steps,\n",
    "        \"n_features\": len(feature_cols)\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple d'utilisation\n",
    "    data_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/data_onchain_merged_tokens_list_high_score.parquet\")\n",
    "    df = pl.read_parquet(data_path)\n",
    "    \n",
    "    print(f\"Données chargées : {len(df)} lignes, {df['token_address'].n_unique()} tokens\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    fe = MemecoinsFeatureEngineer(df)\n",
    "    df_features = fe.create_all_features(forecast_steps=5)\n",
    "    \n",
    "    # Sauvegarder le DataFrame avec features\n",
    "    output_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/memecoin_features_complete.parquet\")\n",
    "    df_features.write_parquet(output_path)\n",
    "    print(f\"\\n💾 Features sauvegardées : {output_path}\")\n",
    "    \n",
    "    # Créer les séquences\n",
    "    print(\"\\nCréation des séquences...\")\n",
    "    sequences_data = create_sequences_from_features(\n",
    "        df_features,\n",
    "        sequence_length=15,\n",
    "        forecast_steps=5,\n",
    "        min_minutes_since_launch=15\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Séquences créées :\")\n",
    "    print(f\"  - Nombre : {len(sequences_data['input_sequences'])}\")\n",
    "    print(f\"  - Shape input : {sequences_data['input_sequences'].shape}\")\n",
    "    print(f\"  - Shape target : {sequences_data['target_sequences'].shape}\")\n",
    "    \n",
    "    # Sauvegarder les séquences\n",
    "    sequences_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_raw.npz\")\n",
    "    np.savez_compressed(sequences_path, **sequences_data)\n",
    "    print(f\"💾 Séquences sauvegardées : {sequences_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "822f1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Données originales: (4933860, 15, 10)\n",
      "📊 Features: ['minutes_since_launch', 'log_price', 'returns', 'price_multiple', 'price_to_ma5_ratio', 'volatility_5m', 'momentum_5m', 'is_pumping', 'is_dumping', 'avg_abs_change_3m']\n",
      "✅ Stats globales calculées pour 10 features\n",
      "🔄 Scaling des séquences...\n",
      "🔄 Scaling des séquences (version rapide)...\n",
      "✅ Scaling terminé!\n",
      "📊 Split: 2783 tokens train, 695 tokens validation\n",
      "\n",
      "✅ Données préparées:\n",
      "  - Train: 3949720 séquences\n",
      "  - Validation: 984140 séquences\n",
      "  - Shape input: (3949720, 15, 10)\n",
      "  - Shape target: (3949720, 5)\n",
      "\n",
      "📊 Statistiques après scaling:\n",
      "  - minutes_since_launch: mean=0.365, std=0.212\n",
      "  - log_price: mean=-0.006, std=0.421\n",
      "  - returns: mean=29442648.000, std=69870747648.000\n",
      "  - price_multiple: mean=3.510, std=7.210\n",
      "  - price_to_ma5_ratio: mean=-0.002, std=0.120\n",
      "\n",
      "💾 Données scalées sauvegardées: /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_scaled.npz\n",
      "💾 Metadata sauvegardés: /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_metadata.json\n",
      "💾 Paramètres du scaler sauvegardés: /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/scaler_params.json\n",
      "\n",
      "📋 Exemple de données:\n",
      "  - Input shape: (15, 10)\n",
      "  - Target shape: (5,)\n",
      "  - Scaling params: ['last_log_price']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data_scaling.py - Version corrigée\n",
    "Scaling et préparation des données pour l'entraînement\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import json\n",
    "\n",
    "class MemecoinSequenceScaler:\n",
    "    \"\"\"\n",
    "    Scaler spécialisé pour les séquences de trading de memecoins\n",
    "    Applique différentes stratégies de scaling selon le type de feature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_method: str = \"robust\"):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.feature_names = None\n",
    "        self.global_stats = {}\n",
    "        self.feature_groups = {\n",
    "            \"price_features\": [\"log_price\"],\n",
    "            \"return_features\": [\"returns\", \"log_returns\", \"volatility_5m\", \"avg_abs_change_3m\"],\n",
    "            \"ratio_features\": [\"price_multiple\", \"price_to_ma5_ratio\", \"momentum_5m\", \"price_range_5m\"],\n",
    "            \"temporal_features\": [\"minutes_since_launch\", \"hour_of_day\", \"minute_of_hour\"],\n",
    "            \"binary_features\": [\"is_pumping\", \"is_dumping\", \"consecutive_ups_3m\"]\n",
    "        }\n",
    "        \n",
    "    def fit_global_stats(self, all_sequences: np.ndarray, feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Calcule les statistiques globales pour certaines features\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Pour chaque type de feature, calculer les stats appropriées\n",
    "        for feat_type, feat_list in self.feature_groups.items():\n",
    "            for feat_name in feat_list:\n",
    "                if feat_name in feature_names:\n",
    "                    idx = feature_names.index(feat_name)\n",
    "                    all_values = all_sequences[:, :, idx].flatten()\n",
    "                    \n",
    "                    # Filtrer les NaN\n",
    "                    valid_values = all_values[~np.isnan(all_values)]\n",
    "                    \n",
    "                    if len(valid_values) > 0:\n",
    "                        self.global_stats[feat_name] = {\n",
    "                            \"min\": float(np.min(valid_values)),\n",
    "                            \"max\": float(np.max(valid_values)),\n",
    "                            \"mean\": float(np.mean(valid_values)),\n",
    "                            \"std\": float(np.std(valid_values)),\n",
    "                            \"median\": float(np.median(valid_values)),\n",
    "                            \"q1\": float(np.percentile(valid_values, 25)),\n",
    "                            \"q3\": float(np.percentile(valid_values, 75))\n",
    "                        }\n",
    "        \n",
    "        print(f\"✅ Stats globales calculées pour {len(self.global_stats)} features\")\n",
    "    \n",
    "    def scale_sequences(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        targets: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Scale toutes les séquences en une fois (vectorisé)\n",
    "        \"\"\"\n",
    "        n_samples, seq_length, n_features = sequences.shape\n",
    "        scaled_sequences = np.zeros_like(sequences)\n",
    "        scaled_targets = np.zeros_like(targets)\n",
    "        \n",
    "        # 1. Prix - Scaling par rapport au dernier point\n",
    "        if \"log_price\" in self.feature_names:\n",
    "            idx = self.feature_names.index(\"log_price\")\n",
    "            # Extraire tous les derniers prix en une fois\n",
    "            last_prices = sequences[:, -1, idx]  # Shape: (n_samples,)\n",
    "            # Broadcaster pour soustraire de chaque timestep\n",
    "            scaled_sequences[:, :, idx] = sequences[:, :, idx] - last_prices[:, np.newaxis]\n",
    "            # Scaler les targets aussi\n",
    "            scaled_targets = targets - last_prices[:, np.newaxis]\n",
    "        \n",
    "        # 2. Returns et volatilité - Calcul vectorisé des stats\n",
    "        for feat_name in self.feature_groups[\"return_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                feat_data = sequences[:, :, idx]  # Shape: (n_samples, seq_length)\n",
    "                \n",
    "                if self.scaling_method == \"robust\":\n",
    "                    # Calcul vectorisé des percentiles par séquence\n",
    "                    q1 = np.nanpercentile(feat_data, 25, axis=1, keepdims=True)\n",
    "                    q3 = np.nanpercentile(feat_data, 75, axis=1, keepdims=True)\n",
    "                    median = np.nanmedian(feat_data, axis=1, keepdims=True)\n",
    "                    iqr = q3 - q1\n",
    "                    \n",
    "                    # Éviter division par zéro\n",
    "                    iqr = np.where(iqr > 1e-8, iqr, self.global_stats.get(feat_name, {}).get(\"std\", 1.0))\n",
    "                    \n",
    "                    scaled_sequences[:, :, idx] = (feat_data - median) / iqr\n",
    "                else:\n",
    "                    # Standard scaling vectorisé\n",
    "                    mean = np.nanmean(feat_data, axis=1, keepdims=True)\n",
    "                    std = np.nanstd(feat_data, axis=1, keepdims=True)\n",
    "                    std = np.where(std > 1e-8, std, 1.0)\n",
    "                    scaled_sequences[:, :, idx] = (feat_data - mean) / std\n",
    "        \n",
    "        # 3. Ratios - Log transform vectorisé\n",
    "        for feat_name in self.feature_groups[\"ratio_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                scaled_sequences[:, :, idx] = np.log(sequences[:, :, idx] + 1e-8)\n",
    "        \n",
    "        # 4. Features temporelles - Normalisation globale vectorisée\n",
    "        for feat_name in self.feature_groups[\"temporal_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                if feat_name == \"minutes_since_launch\" and feat_name in self.global_stats:\n",
    "                    stats = self.global_stats[feat_name]\n",
    "                    scaled_sequences[:, :, idx] = (sequences[:, :, idx] - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"] + 1e-8)\n",
    "                elif feat_name == \"hour_of_day\":\n",
    "                    scaled_sequences[:, :, idx] = sequences[:, :, idx] / 24.0\n",
    "                elif feat_name == \"minute_of_hour\":\n",
    "                    scaled_sequences[:, :, idx] = sequences[:, :, idx] / 60.0\n",
    "        \n",
    "        # 5. Features binaires - Copie directe\n",
    "        for feat_name in self.feature_groups[\"binary_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                scaled_sequences[:, :, idx] = sequences[:, :, idx]\n",
    "        \n",
    "        # Remplacer NaN\n",
    "        scaled_sequences = np.nan_to_num(scaled_sequences, nan=0.0)\n",
    "        scaled_targets = np.nan_to_num(scaled_targets, nan=0.0)\n",
    "        \n",
    "        # Créer les params (simplifié pour la version batch)\n",
    "        scaling_params = [{\n",
    "            \"last_log_price\": float(last_prices[i]) if \"log_price\" in self.feature_names else 0.0\n",
    "        } for i in range(n_samples)]\n",
    "        \n",
    "        return scaled_sequences, scaled_targets, scaling_params\n",
    "    \n",
    "    def inverse_scale_predictions(\n",
    "        self, \n",
    "        predictions: np.ndarray, \n",
    "        scaling_params: Dict\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Inverse le scaling pour les prédictions\n",
    "        \"\"\"\n",
    "        if \"last_log_price\" in scaling_params:\n",
    "            # Les prédictions sont des différences par rapport au dernier log_price\n",
    "            return predictions + scaling_params[\"last_log_price\"]\n",
    "        return predictions\n",
    "    \n",
    "    def save_scaler(self, path: Path):\n",
    "        \"\"\"Sauvegarde les paramètres du scaler\"\"\"\n",
    "        scaler_data = {\n",
    "            \"scaling_method\": self.scaling_method,\n",
    "            \"feature_names\": self.feature_names,\n",
    "            \"global_stats\": self.global_stats,\n",
    "            \"feature_groups\": self.feature_groups\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(scaler_data, f, indent=2)\n",
    "    \n",
    "    def load_scaler(self, path: Path):\n",
    "        \"\"\"Charge les paramètres du scaler\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            scaler_data = json.load(f)\n",
    "        self.scaling_method = scaler_data[\"scaling_method\"]\n",
    "        self.feature_names = scaler_data[\"feature_names\"]\n",
    "        self.global_stats = scaler_data[\"global_stats\"]\n",
    "        self.feature_groups = scaler_data[\"feature_groups\"]\n",
    "\n",
    "\n",
    "class DataPreparer:\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement avec train/val split et scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences_path: Path):\n",
    "        # Charger les séquences avec allow_pickle=True pour les metadata\n",
    "        data = np.load(sequences_path, allow_pickle=True)\n",
    "        \n",
    "        # Convertir en dictionnaire normal\n",
    "        self.sequences_data = {}\n",
    "        for key in data.files:\n",
    "            self.sequences_data[key] = data[key]\n",
    "            \n",
    "        # Convertir les types si nécessaire\n",
    "        if \"feature_names\" in self.sequences_data and isinstance(self.sequences_data[\"feature_names\"], np.ndarray):\n",
    "            if self.sequences_data[\"feature_names\"].ndim == 0:\n",
    "                # Cas où c'est un scalar array contenant une liste\n",
    "                self.sequences_data[\"feature_names\"] = self.sequences_data[\"feature_names\"].item()\n",
    "            else:\n",
    "                self.sequences_data[\"feature_names\"] = self.sequences_data[\"feature_names\"].tolist()\n",
    "                \n",
    "        # Convertir les autres champs scalaires si nécessaire\n",
    "        for key in [\"sequence_length\", \"forecast_steps\", \"n_features\"]:\n",
    "            if key in self.sequences_data and isinstance(self.sequences_data[key], np.ndarray):\n",
    "                self.sequences_data[key] = int(self.sequences_data[key].item())\n",
    "        \n",
    "        self.scaler = MemecoinSequenceScaler(scaling_method=\"robust\")\n",
    "        \n",
    "    def prepare_for_training(\n",
    "        self, \n",
    "        validation_split: float = 0.2,\n",
    "        random_seed: int = 42\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Prépare les données complètes pour l'entraînement\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        # Extraire les données\n",
    "        input_sequences = self.sequences_data[\"input_sequences\"]\n",
    "        target_sequences = self.sequences_data[\"target_sequences\"]\n",
    "        metadata = self.sequences_data[\"metadata\"]\n",
    "        \n",
    "        # Gérer les feature_names selon le format\n",
    "        if isinstance(self.sequences_data[\"feature_names\"], list):\n",
    "            feature_names = self.sequences_data[\"feature_names\"]\n",
    "        else:\n",
    "            feature_names = self.sequences_data[\"feature_names\"].tolist()\n",
    "        \n",
    "        print(f\"📊 Données originales: {input_sequences.shape}\")\n",
    "        print(f\"📊 Features: {feature_names}\")\n",
    "        \n",
    "        # Fit le scaler sur toutes les données\n",
    "        self.scaler.fit_global_stats(input_sequences, feature_names)\n",
    "        \n",
    "        # Scaler toutes les séquences\n",
    "        print(\"🔄 Scaling des séquences...\")\n",
    "        scaled_inputs = []\n",
    "        scaled_targets = []\n",
    "        all_scaling_params = []\n",
    "        \n",
    "        # for i in range(len(input_sequences)):\n",
    "        #     scaled_input, scaled_target, params = self.scaler.scale_sequence(\n",
    "        #         input_sequences[i], \n",
    "        #         target_sequences[i]\n",
    "        #     )\n",
    "        #     scaled_inputs.append(scaled_input)\n",
    "        #     scaled_targets.append(scaled_target)\n",
    "        #     all_scaling_params.append(params)\n",
    "\n",
    "        print(\"🔄 Scaling des séquences (version rapide)...\")\n",
    "        scaled_inputs, scaled_targets, all_scaling_params = self.scaler.scale_sequences(\n",
    "            input_sequences, \n",
    "            target_sequences\n",
    "        )\n",
    "        print(\"✅ Scaling terminé!\")\n",
    "\n",
    "        scaled_inputs = np.array(scaled_inputs)\n",
    "        scaled_targets = np.array(scaled_targets)\n",
    "        \n",
    "        # Split par token pour éviter le data leakage\n",
    "        unique_tokens = list(set([m[\"token\"] for m in metadata]))\n",
    "        np.random.shuffle(unique_tokens)\n",
    "        \n",
    "        n_val_tokens = int(len(unique_tokens) * validation_split)\n",
    "        val_tokens = set(unique_tokens[:n_val_tokens])\n",
    "        \n",
    "        print(f\"📊 Split: {len(unique_tokens)-n_val_tokens} tokens train, {n_val_tokens} tokens validation\")\n",
    "        \n",
    "        # Créer les indices\n",
    "        train_idx = [i for i, m in enumerate(metadata) if m[\"token\"] not in val_tokens]\n",
    "        val_idx = [i for i, m in enumerate(metadata) if m[\"token\"] in val_tokens]\n",
    "        \n",
    "        # Créer les datasets\n",
    "        train_data = {\n",
    "            \"inputs\": scaled_inputs[train_idx],\n",
    "            \"targets\": scaled_targets[train_idx],\n",
    "            \"metadata\": [metadata[i] for i in train_idx],\n",
    "            \"scaling_params\": [all_scaling_params[i] for i in train_idx]\n",
    "        }\n",
    "        \n",
    "        val_data = {\n",
    "            \"inputs\": scaled_inputs[val_idx],\n",
    "            \"targets\": scaled_targets[val_idx],\n",
    "            \"metadata\": [metadata[i] for i in val_idx],\n",
    "            \"scaling_params\": [all_scaling_params[i] for i in val_idx]\n",
    "        }\n",
    "        \n",
    "        # Statistiques\n",
    "        print(f\"\\n✅ Données préparées:\")\n",
    "        print(f\"  - Train: {len(train_data['inputs'])} séquences\")\n",
    "        print(f\"  - Validation: {len(val_data['inputs'])} séquences\")\n",
    "        print(f\"  - Shape input: {train_data['inputs'].shape}\")\n",
    "        print(f\"  - Shape target: {train_data['targets'].shape}\")\n",
    "        \n",
    "        # Vérifier les distributions\n",
    "        print(f\"\\n📊 Statistiques après scaling:\")\n",
    "        for i, feat in enumerate(feature_names[:5]):  # Top 5 features\n",
    "            train_values = train_data[\"inputs\"][:, :, i].flatten()\n",
    "            print(f\"  - {feat}: mean={np.mean(train_values):.3f}, std={np.std(train_values):.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"train\": train_data,\n",
    "            \"validation\": val_data,\n",
    "            \"scaler\": self.scaler,\n",
    "            \"feature_names\": feature_names,\n",
    "            \"metadata\": {\n",
    "                \"sequence_length\": self.sequences_data.get(\"sequence_length\", 15),\n",
    "                \"forecast_steps\": self.sequences_data.get(\"forecast_steps\", 5),\n",
    "                \"n_features\": self.sequences_data.get(\"n_features\", len(feature_names))\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chemins\n",
    "    sequences_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_raw.npz\")\n",
    "    output_dir = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/\")\n",
    "    \n",
    "    # Préparer les données\n",
    "    preparer = DataPreparer(sequences_path)\n",
    "    prepared_data = preparer.prepare_for_training(validation_split=0.2)\n",
    "    \n",
    "    # Sauvegarder les données scalées - SANS les metadata objects\n",
    "    output_path = output_dir / \"sequences_scaled.npz\"\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        train_inputs=prepared_data[\"train\"][\"inputs\"],\n",
    "        train_targets=prepared_data[\"train\"][\"targets\"],\n",
    "        val_inputs=prepared_data[\"validation\"][\"inputs\"],\n",
    "        val_targets=prepared_data[\"validation\"][\"targets\"],\n",
    "        feature_names=prepared_data[\"feature_names\"],\n",
    "        **prepared_data[\"metadata\"]\n",
    "    )\n",
    "    print(f\"\\n💾 Données scalées sauvegardées: {output_path}\")\n",
    "    \n",
    "    # Sauvegarder les metadata séparément en JSON\n",
    "    metadata_path = output_dir / \"sequences_metadata.json\"\n",
    "    metadata_to_save = {\n",
    "        \"train_tokens\": list(set([m[\"token\"] for m in prepared_data[\"train\"][\"metadata\"]])),\n",
    "        \"val_tokens\": list(set([m[\"token\"] for m in prepared_data[\"validation\"][\"metadata\"]])),\n",
    "        \"n_train_sequences\": len(prepared_data[\"train\"][\"inputs\"]),\n",
    "        \"n_val_sequences\": len(prepared_data[\"validation\"][\"inputs\"]),\n",
    "        \"feature_names\": prepared_data[\"feature_names\"],\n",
    "        **prepared_data[\"metadata\"]\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata_to_save, f, indent=2)\n",
    "    print(f\"💾 Metadata sauvegardés: {metadata_path}\")\n",
    "    \n",
    "    # Sauvegarder le scaler\n",
    "    scaler_path = output_dir / \"scaler_params.json\"\n",
    "    prepared_data[\"scaler\"].save_scaler(scaler_path)\n",
    "    print(f\"💾 Paramètres du scaler sauvegardés: {scaler_path}\")\n",
    "    \n",
    "    # Exemple d'utilisation\n",
    "    print(\"\\n📋 Exemple de données:\")\n",
    "    example_idx = 0\n",
    "    print(f\"  - Input shape: {prepared_data['train']['inputs'][example_idx].shape}\")\n",
    "    print(f\"  - Target shape: {prepared_data['train']['targets'][example_idx].shape}\")\n",
    "    print(f\"  - Scaling params: {list(prepared_data['train']['scaling_params'][example_idx].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61241cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5984cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
