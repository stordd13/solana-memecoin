{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d3c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bc82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = Path(\n",
    "        \"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/data_onchain_merged_tokens_list.parquet\"\n",
    "    )\n",
    "\n",
    "lf_raw = pl.scan_parquet(\n",
    "        data_file_path,\n",
    "        low_memory=False,\n",
    "        cache=False,\n",
    "        use_statistics=True,\n",
    "        parallel=\"auto\",\n",
    "        rechunk=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee2b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (66_494_523, 36)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_utc</th><th>price</th><th>token_address</th><th>token_name</th><th>token_symbol</th><th>bonding_curve_address</th><th>mint_authority</th><th>signature</th><th>decimals</th><th>metadata_uri</th><th>initial_supply</th><th>creator</th><th>creators</th><th>is_mutable</th><th>virtual_token_reserves</th><th>virtual_sol_reserves</th><th>real_token_reserves</th><th>virtual_token_reserves.buy</th><th>virtual_sol_reserves.buy</th><th>real_sol_reserves.buy</th><th>real_token_reserves.buy</th><th>seller_fee_basis</th><th>fee.buy</th><th>fee_recipient.buy</th><th>fee_basis_points.buy</th><th>slot</th><th>block_time</th><th>created_at</th><th>metadata_name</th><th>metadata_symbol</th><th>metadata_description</th><th>metadata_image</th><th>metadata_show_name</th><th>metadata_created_on</th><th>metadata_website</th><th>metadata_twitter</th></tr><tr><td>datetime[Î¼s]</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>2025-05-17 23:50:00</td><td>3.8284e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:51:00</td><td>3.8284e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:52:00</td><td>4.0220e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:53:00</td><td>3.7744e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2025-05-17 23:54:00</td><td>3.7744e-7</td><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2025-06-28 11:38:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Seâ€¦</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JAâ€¦</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6â€¦</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreiggâ€¦</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySyâ€¦</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53Eâ€¦</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8â€¦</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiaeâ€¦</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?â€¦</td><td>null</td></tr><tr><td>2025-06-28 11:39:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Seâ€¦</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JAâ€¦</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6â€¦</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreiggâ€¦</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySyâ€¦</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53Eâ€¦</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8â€¦</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiaeâ€¦</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?â€¦</td><td>null</td></tr><tr><td>2025-06-28 11:40:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Seâ€¦</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JAâ€¦</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6â€¦</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreiggâ€¦</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySyâ€¦</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53Eâ€¦</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8â€¦</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiaeâ€¦</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?â€¦</td><td>null</td></tr><tr><td>2025-06-28 11:41:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Seâ€¦</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JAâ€¦</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6â€¦</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreiggâ€¦</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySyâ€¦</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53Eâ€¦</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8â€¦</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiaeâ€¦</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?â€¦</td><td>null</td></tr><tr><td>2025-06-28 11:42:00</td><td>0.000004</td><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;5n7abcNhqkqwdJ79X3DmPoRyiTv3Seâ€¦</td><td>&quot;TSLvdd1pWpHVjahSpsvCXUbgwsL3JAâ€¦</td><td>&quot;27t5VuXxE9MBVrnKLsfW5oqcYDVEm6â€¦</td><td>6.0</td><td>&quot;https://ipfs.io/ipfs/bafkreiggâ€¦</td><td>1.0000e15</td><td>&quot;DaPHw3Aa1wbzC53EmToqS1XnywJySyâ€¦</td><td>&quot;[{&quot;address&quot;: &quot;DaPHw3Aa1wbzC53Eâ€¦</td><td>false</td><td>1.0730e15</td><td>3.0000e10</td><td>7.9310e14</td><td>&quot;1006560371542270&quot;</td><td>&quot;31980198019&quot;</td><td>&quot;1980198019&quot;</td><td>&quot;726660371542270&quot;</td><td>0.0</td><td>&quot;18811882&quot;</td><td>&quot;9rPYyANsfQZw3DnDmKE3YCQF5E8oD8â€¦</td><td>&quot;95&quot;</td><td>3.4942927e8</td><td>1.7510e9</td><td>&quot;27-06-2025 02:24:02&quot;</td><td>&quot;you did not search for this coâ€¦</td><td>&quot;it found u&quot;</td><td>&quot;&quot;</td><td>&quot;https://ipfs.io/ipfs/bafkreiaeâ€¦</td><td>true</td><td>&quot;https://pump.fun&quot;</td><td>&quot;https://www.youtube.com/watch?â€¦</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (66_494_523, 36)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ date_utc  â”† price     â”† token_add â”† token_nam â”† â€¦ â”† metadata_ â”† metadata_ â”† metadata_ â”† metadata â”‚\n",
       "â”‚ ---       â”† ---       â”† ress      â”† e         â”†   â”† show_name â”† created_o â”† website   â”† _twitter â”‚\n",
       "â”‚ datetime[ â”† f64       â”† ---       â”† ---       â”†   â”† ---       â”† n         â”† ---       â”† ---      â”‚\n",
       "â”‚ Î¼s]       â”†           â”† str       â”† str       â”†   â”† bool      â”† ---       â”† str       â”† str      â”‚\n",
       "â”‚           â”†           â”†           â”†           â”†   â”†           â”† str       â”†           â”†          â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2025-05-1 â”† 3.8284e-7 â”† 1kMzbW3Jt â”† null      â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 7         â”†           â”† fCQgXRjQh â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 23:50:00  â”†           â”† bFkEDtAaZ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”†           â”† DYAâ€¦      â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-05-1 â”† 3.8284e-7 â”† 1kMzbW3Jt â”† null      â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 7         â”†           â”† fCQgXRjQh â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 23:51:00  â”†           â”† bFkEDtAaZ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”†           â”† DYAâ€¦      â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-05-1 â”† 4.0220e-7 â”† 1kMzbW3Jt â”† null      â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 7         â”†           â”† fCQgXRjQh â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 23:52:00  â”†           â”† bFkEDtAaZ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”†           â”† DYAâ€¦      â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-05-1 â”† 3.7744e-7 â”† 1kMzbW3Jt â”† null      â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 7         â”†           â”† fCQgXRjQh â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 23:53:00  â”†           â”† bFkEDtAaZ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”†           â”† DYAâ€¦      â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-05-1 â”† 3.7744e-7 â”† 1kMzbW3Jt â”† null      â”† â€¦ â”† null      â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 7         â”†           â”† fCQgXRjQh â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 23:54:00  â”†           â”† bFkEDtAaZ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”†           â”† DYAâ€¦      â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ â€¦         â”† â€¦         â”† â€¦         â”† â€¦         â”† â€¦ â”† â€¦         â”† â€¦         â”† â€¦         â”† â€¦        â”‚\n",
       "â”‚ 2025-06-2 â”† 0.000004  â”† zsAM6khYe â”† you did   â”† â€¦ â”† true      â”† https://p â”† https://w â”† null     â”‚\n",
       "â”‚ 8         â”†           â”† 7UTjhDkKb â”† not       â”†   â”†           â”† ump.fun   â”† ww.youtub â”†          â”‚\n",
       "â”‚ 11:38:00  â”†           â”† JtPeQwicH â”† search    â”†   â”†           â”†           â”† e.com/wat â”†          â”‚\n",
       "â”‚           â”†           â”† ZtHâ€¦      â”† for this  â”†   â”†           â”†           â”† ch?â€¦      â”†          â”‚\n",
       "â”‚           â”†           â”†           â”† coâ€¦       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-06-2 â”† 0.000004  â”† zsAM6khYe â”† you did   â”† â€¦ â”† true      â”† https://p â”† https://w â”† null     â”‚\n",
       "â”‚ 8         â”†           â”† 7UTjhDkKb â”† not       â”†   â”†           â”† ump.fun   â”† ww.youtub â”†          â”‚\n",
       "â”‚ 11:39:00  â”†           â”† JtPeQwicH â”† search    â”†   â”†           â”†           â”† e.com/wat â”†          â”‚\n",
       "â”‚           â”†           â”† ZtHâ€¦      â”† for this  â”†   â”†           â”†           â”† ch?â€¦      â”†          â”‚\n",
       "â”‚           â”†           â”†           â”† coâ€¦       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-06-2 â”† 0.000004  â”† zsAM6khYe â”† you did   â”† â€¦ â”† true      â”† https://p â”† https://w â”† null     â”‚\n",
       "â”‚ 8         â”†           â”† 7UTjhDkKb â”† not       â”†   â”†           â”† ump.fun   â”† ww.youtub â”†          â”‚\n",
       "â”‚ 11:40:00  â”†           â”† JtPeQwicH â”† search    â”†   â”†           â”†           â”† e.com/wat â”†          â”‚\n",
       "â”‚           â”†           â”† ZtHâ€¦      â”† for this  â”†   â”†           â”†           â”† ch?â€¦      â”†          â”‚\n",
       "â”‚           â”†           â”†           â”† coâ€¦       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-06-2 â”† 0.000004  â”† zsAM6khYe â”† you did   â”† â€¦ â”† true      â”† https://p â”† https://w â”† null     â”‚\n",
       "â”‚ 8         â”†           â”† 7UTjhDkKb â”† not       â”†   â”†           â”† ump.fun   â”† ww.youtub â”†          â”‚\n",
       "â”‚ 11:41:00  â”†           â”† JtPeQwicH â”† search    â”†   â”†           â”†           â”† e.com/wat â”†          â”‚\n",
       "â”‚           â”†           â”† ZtHâ€¦      â”† for this  â”†   â”†           â”†           â”† ch?â€¦      â”†          â”‚\n",
       "â”‚           â”†           â”†           â”† coâ€¦       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 2025-06-2 â”† 0.000004  â”† zsAM6khYe â”† you did   â”† â€¦ â”† true      â”† https://p â”† https://w â”† null     â”‚\n",
       "â”‚ 8         â”†           â”† 7UTjhDkKb â”† not       â”†   â”†           â”† ump.fun   â”† ww.youtub â”†          â”‚\n",
       "â”‚ 11:42:00  â”†           â”† JtPeQwicH â”† search    â”†   â”†           â”†           â”† e.com/wat â”†          â”‚\n",
       "â”‚           â”†           â”† ZtHâ€¦      â”† for this  â”†   â”†           â”†           â”† ch?â€¦      â”†          â”‚\n",
       "â”‚           â”†           â”†           â”† coâ€¦       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_raw.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38ffba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_21835/3122231484.py:1: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  lf_raw.columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['date_utc',\n",
       " 'price',\n",
       " 'token_address',\n",
       " 'token_name',\n",
       " 'token_symbol',\n",
       " 'bonding_curve_address',\n",
       " 'mint_authority',\n",
       " 'signature',\n",
       " 'decimals',\n",
       " 'metadata_uri',\n",
       " 'initial_supply',\n",
       " 'creator',\n",
       " 'creators',\n",
       " 'is_mutable',\n",
       " 'virtual_token_reserves',\n",
       " 'virtual_sol_reserves',\n",
       " 'real_token_reserves',\n",
       " 'virtual_token_reserves.buy',\n",
       " 'virtual_sol_reserves.buy',\n",
       " 'real_sol_reserves.buy',\n",
       " 'real_token_reserves.buy',\n",
       " 'seller_fee_basis',\n",
       " 'fee.buy',\n",
       " 'fee_recipient.buy',\n",
       " 'fee_basis_points.buy',\n",
       " 'slot',\n",
       " 'block_time',\n",
       " 'created_at',\n",
       " 'metadata_name',\n",
       " 'metadata_symbol',\n",
       " 'metadata_description',\n",
       " 'metadata_image',\n",
       " 'metadata_show_name',\n",
       " 'metadata_created_on',\n",
       " 'metadata_website',\n",
       " 'metadata_twitter']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98b40650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/948930100.py:5: DeprecationWarning: the `streaming` parameter was deprecated in 1.25.0; use `engine` instead.\n",
      "  .collect(streaming=True)\n"
     ]
    }
   ],
   "source": [
    "lf = lf_raw.select(\"token_address\", \"date_utc\", \"price\")\n",
    "tokens_address = (\n",
    "    lf.select(\"token_address\")\n",
    "    .unique()\n",
    "    .collect(streaming=True)\n",
    "    .get_column(\"token_address\")\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e9471ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (66_494_523, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token_address</th><th>date_utc</th><th>price</th></tr><tr><td>str</td><td>datetime[Î¼s]</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>2025-05-17 23:50:00</td><td>3.8284e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>2025-05-17 23:51:00</td><td>3.8284e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>2025-05-17 23:52:00</td><td>4.0220e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>2025-05-17 23:53:00</td><td>3.7744e-7</td></tr><tr><td>&quot;1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦</td><td>2025-05-17 23:54:00</td><td>3.7744e-7</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>2025-06-28 11:38:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>2025-06-28 11:39:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>2025-06-28 11:40:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>2025-06-28 11:41:00</td><td>0.000004</td></tr><tr><td>&quot;zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦</td><td>2025-06-28 11:42:00</td><td>0.000004</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (66_494_523, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ token_address                   â”† date_utc            â”† price     â”‚\n",
       "â”‚ ---                             â”† ---                 â”† ---       â”‚\n",
       "â”‚ str                             â”† datetime[Î¼s]        â”† f64       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦ â”† 2025-05-17 23:50:00 â”† 3.8284e-7 â”‚\n",
       "â”‚ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦ â”† 2025-05-17 23:51:00 â”† 3.8284e-7 â”‚\n",
       "â”‚ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦ â”† 2025-05-17 23:52:00 â”† 4.0220e-7 â”‚\n",
       "â”‚ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦ â”† 2025-05-17 23:53:00 â”† 3.7744e-7 â”‚\n",
       "â”‚ 1kMzbW3JtfCQgXRjQhbFkEDtAaZDYAâ€¦ â”† 2025-05-17 23:54:00 â”† 3.7744e-7 â”‚\n",
       "â”‚ â€¦                               â”† â€¦                   â”† â€¦         â”‚\n",
       "â”‚ zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦ â”† 2025-06-28 11:38:00 â”† 0.000004  â”‚\n",
       "â”‚ zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦ â”† 2025-06-28 11:39:00 â”† 0.000004  â”‚\n",
       "â”‚ zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦ â”† 2025-06-28 11:40:00 â”† 0.000004  â”‚\n",
       "â”‚ zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦ â”† 2025-06-28 11:41:00 â”† 0.000004  â”‚\n",
       "â”‚ zsAM6khYe7UTjhDkKbJtPeQwicHZtHâ€¦ â”† 2025-06-28 11:42:00 â”† 0.000004  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2afb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "\n",
    "def compute_log_returns(df: pl.DataFrame) -> pl.Series | None:\n",
    "    \"\"\"Calcule les log-returns et retourne une Series Polars.\"\"\"\n",
    "    if df.height < 2:\n",
    "        return None\n",
    "    log_returns = (\n",
    "        df.select(\n",
    "            pl.col(\"price\")\n",
    "            .log().diff()\n",
    "            .alias(\"log_returns\")\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .get_column(\"log_returns\")\n",
    "    )\n",
    "    return log_returns if not log_returns.is_empty() else None\n",
    "\n",
    "def compute_volatility_log_return(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\"Calcule la volatilitÃ© (Ã©cart-type) des log-returns.\"\"\"\n",
    "    log_returns = compute_log_returns(df)\n",
    "    if log_returns is None:\n",
    "        return None\n",
    "    return float(log_returns.std())\n",
    "\n",
    "def compute_kurtosis_log_return(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\"Calcule le kurtosis des log-returns.\"\"\"\n",
    "    log_returns = compute_log_returns(df)\n",
    "    if log_returns is None:\n",
    "        return None\n",
    "    kurtosis_value = float(kurtosis(log_returns.to_numpy().flatten(), fisher=True, bias=False))\n",
    "    return max(1E-6, kurtosis_value)\n",
    "\n",
    "def compute_price_ratio(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\"Calcule le ratio prix max / prix min.\"\"\"\n",
    "    min_price, max_price = float(df[\"price\"].min()), float(df[\"price\"].max())\n",
    "    return min(1E6, max_price / (min_price + 1E-9))\n",
    "\n",
    "def compute_score(df: pl.DataFrame) -> float | None:\n",
    "    \"\"\" Calcule un score normalisÃ© entre 0 (mauvais) et 1 (excellent). \"\"\"\n",
    "    if df.is_empty():\n",
    "        return None\n",
    "    df = df.sort(\"date_utc\")\n",
    "    price_ratio = min(100, compute_price_ratio(df))\n",
    "    if price_ratio == 1:\n",
    "        return 0.0\n",
    "    kurtosis_log_return = compute_kurtosis_log_return(df)\n",
    "    volatility_log_return = compute_volatility_log_return(df)\n",
    "\n",
    "    if kurtosis_log_return is None or volatility_log_return is None:\n",
    "        return None\n",
    "\n",
    "    score = (volatility_log_return / (kurtosis_log_return + 1E-9)) * (price_ratio - 1)\n",
    "    score = (np.exp(score) - 1) * (price_ratio - 1)\n",
    "    score = abs(score / (1 + score))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de880aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "507da7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5_072_980, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token_address</th><th>date_utc</th><th>price</th></tr><tr><td>str</td><td>datetime[Î¼s]</td><td>f64</td></tr></thead><tbody><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:07:00</td><td>0.00085</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:08:00</td><td>0.001856</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:09:00</td><td>0.001977</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:10:00</td><td>0.002113</td></tr><tr><td>&quot;1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY&quot;</td><td>2025-05-15 06:11:00</td><td>714.986041</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:02:00</td><td>0.000066</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:03:00</td><td>0.000066</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:04:00</td><td>0.000066</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:05:00</td><td>0.000067</td></tr><tr><td>&quot;zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL&quot;</td><td>2025-02-14 02:06:00</td><td>0.000067</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5_072_980, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ token_address                               â”† date_utc            â”† price      â”‚\n",
       "â”‚ ---                                         â”† ---                 â”† ---        â”‚\n",
       "â”‚ str                                         â”† datetime[Î¼s]        â”† f64        â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY â”† 2025-05-15 06:07:00 â”† 0.00085    â”‚\n",
       "â”‚ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY â”† 2025-05-15 06:08:00 â”† 0.001856   â”‚\n",
       "â”‚ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY â”† 2025-05-15 06:09:00 â”† 0.001977   â”‚\n",
       "â”‚ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY â”† 2025-05-15 06:10:00 â”† 0.002113   â”‚\n",
       "â”‚ 1sGQMfgWZqN4gmobUmGcm76rJiuMTB8gffr7kTJchPY â”† 2025-05-15 06:11:00 â”† 714.986041 â”‚\n",
       "â”‚ â€¦                                           â”† â€¦                   â”† â€¦          â”‚\n",
       "â”‚ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL â”† 2025-02-14 02:02:00 â”† 0.000066   â”‚\n",
       "â”‚ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL â”† 2025-02-14 02:03:00 â”† 0.000066   â”‚\n",
       "â”‚ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL â”† 2025-02-14 02:04:00 â”† 0.000066   â”‚\n",
       "â”‚ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL â”† 2025-02-14 02:05:00 â”† 0.000067   â”‚\n",
       "â”‚ zMTRXd5vbtUninsU32Y1e6htZg8FJQVFNfYhR9bPooL â”† 2025-02-14 02:06:00 â”† 0.000067   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_parquet(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/data_onchain_merged_tokens_list_high_score.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5b019940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token_address</th><th>date_utc</th><th>price</th></tr><tr><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>5072980</td><td>5072980</td><td>5072980</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ token_address â”† date_utc â”† price   â”‚\n",
       "â”‚ ---           â”† ---      â”† ---     â”‚\n",
       "â”‚ u32           â”† u32      â”† u32     â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 5072980       â”† 5072980  â”† 5072980 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "531472d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DonnÃ©es chargÃ©es : 5072980 lignes, 3478 tokens\n",
      "1. Ajout des features de base...\n",
      "2. Ajout des indicateurs techniques rapides...\n",
      "3. Ajout des features de dÃ©tection pump/dump...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:50: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_mean(3, min_periods=1).over(\"token_address\").alias(\"ma_3\"),\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:51: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\").alias(\"ma_5\"),\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:52: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_mean(10, min_periods=5).over(\"token_address\").alias(\"ma_10\"),\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:55: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  (pl.col(\"price\") / pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\"))\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:59: DeprecationWarning: the argument `min_periods` for `Expr.rolling_std` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"log_returns\").rolling_std(5, min_periods=2).over(\"token_address\")\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:63: DeprecationWarning: the argument `min_periods` for `Expr.rolling_max` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  (pl.col(\"price\").rolling_max(5, min_periods=2).over(\"token_address\") /\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:64: DeprecationWarning: the argument `min_periods` for `Expr.rolling_min` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  pl.col(\"price\").rolling_min(5, min_periods=2).over(\"token_address\"))\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:78: DeprecationWarning: the argument `min_periods` for `Expr.rolling_sum` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  .rolling_sum(3, min_periods=1).over(\"token_address\")\n",
      "/var/folders/nk/n3qvd0rj1mbffbxm9x6xm3q00000gn/T/ipykernel_22033/3286295278.py:83: DeprecationWarning: the argument `min_periods` for `Expr.rolling_mean` is deprecated. It was renamed to `min_samples` in version 1.21.0.\n",
      "  .rolling_mean(3, min_periods=1).over(\"token_address\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Ajout des targets multi-step...\n",
      "âœ… Feature engineering terminÃ©: 45 colonnes\n",
      "\n",
      "ğŸ’¾ Features sauvegardÃ©es : /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/memecoin_features_complete.parquet\n",
      "\n",
      "CrÃ©ation des sÃ©quences...\n",
      "\n",
      "âœ… SÃ©quences crÃ©Ã©es :\n",
      "  - Nombre : 4933860\n",
      "  - Shape input : (4933860, 15, 10)\n",
      "  - Shape target : (4933860, 5)\n",
      "ğŸ’¾ SÃ©quences sauvegardÃ©es : /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_raw.npz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "feature_engineering.py\n",
    "Feature engineering pour les donnÃ©es de memecoins\n",
    "\"\"\"\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class MemecoinsFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering spÃ©cialisÃ© pour les memecoins\n",
    "    Transforme les donnÃ©es brutes (price, date_utc, token_address) \n",
    "    en features utilisables pour le ML\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pl.DataFrame):\n",
    "        self.df = df.sort([\"token_address\", \"date_utc\"])\n",
    "        \n",
    "    def add_core_features(self) -> pl.DataFrame:\n",
    "        \"\"\"Features essentielles calculables dÃ¨s les premiÃ¨res minutes\"\"\"\n",
    "        self.df = self.df.with_columns([\n",
    "            # 1. Features temporelles basiques\n",
    "            pl.col(\"date_utc\").dt.minute().alias(\"minute_of_hour\"),\n",
    "            pl.col(\"date_utc\").dt.hour().alias(\"hour_of_day\"),\n",
    "            \n",
    "            # 2. Minutes depuis le lancement (crucial pour les memecoins)\n",
    "            (pl.col(\"date_utc\") - pl.col(\"date_utc\").first().over(\"token_address\"))\n",
    "            .dt.total_minutes()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(\"minutes_since_launch\"),\n",
    "            \n",
    "            # 3. Prix transformÃ© (plus stable)\n",
    "            pl.col(\"price\").log().alias(\"log_price\"),\n",
    "            \n",
    "            # 4. Returns instantanÃ©s\n",
    "            pl.col(\"price\").pct_change().over(\"token_address\").fill_null(0).alias(\"returns\"),\n",
    "            pl.col(\"price\").log().diff().over(\"token_address\").fill_null(0).alias(\"log_returns\"),\n",
    "            \n",
    "            # 5. Prix relatif au prix initial\n",
    "            (pl.col(\"price\") / pl.col(\"price\").first().over(\"token_address\")).alias(\"price_multiple\"),\n",
    "        ])\n",
    "        return self.df\n",
    "    \n",
    "    def add_fast_indicators(self) -> pl.DataFrame:\n",
    "        \"\"\"Indicateurs calculables avec peu de points\"\"\"\n",
    "        self.df = self.df.with_columns([\n",
    "            # Rolling windows courts (3, 5, 10 minutes max)\n",
    "            pl.col(\"price\").rolling_mean(3, min_periods=1).over(\"token_address\").alias(\"ma_3\"),\n",
    "            pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\").alias(\"ma_5\"),\n",
    "            pl.col(\"price\").rolling_mean(10, min_periods=5).over(\"token_address\").alias(\"ma_10\"),\n",
    "            \n",
    "            # Ã‰cart au MA\n",
    "            (pl.col(\"price\") / pl.col(\"price\").rolling_mean(5, min_periods=2).over(\"token_address\"))\n",
    "            .fill_null(1.0).alias(\"price_to_ma5_ratio\"),\n",
    "            \n",
    "            # VolatilitÃ© instantanÃ©e (rolling sur 5 minutes)\n",
    "            pl.col(\"log_returns\").rolling_std(5, min_periods=2).over(\"token_address\")\n",
    "            .fill_null(0).alias(\"volatility_5m\"),\n",
    "            \n",
    "            # Range de prix sur 5 minutes\n",
    "            (pl.col(\"price\").rolling_max(5, min_periods=2).over(\"token_address\") / \n",
    "             pl.col(\"price\").rolling_min(5, min_periods=2).over(\"token_address\"))\n",
    "            .fill_null(1.0).alias(\"price_range_5m\"),\n",
    "            \n",
    "            # Momentum simple\n",
    "            (pl.col(\"price\") / pl.col(\"price\").shift(5).over(\"token_address\"))\n",
    "            .fill_null(1.0).alias(\"momentum_5m\"),\n",
    "        ])\n",
    "        return self.df\n",
    "    \n",
    "    def add_pump_detection_features(self) -> pl.DataFrame:\n",
    "        \"\"\"Features spÃ©cifiques pour dÃ©tecter les pumps/dumps\"\"\"\n",
    "        self.df = self.df.with_columns([\n",
    "            # Nombre de hausses consÃ©cutives\n",
    "            (pl.col(\"returns\") > 0).cast(pl.Int32)\n",
    "            .rolling_sum(3, min_periods=1).over(\"token_address\")\n",
    "            .alias(\"consecutive_ups_3m\"),\n",
    "            \n",
    "            # Vitesse de changement\n",
    "            pl.col(\"log_returns\").abs()\n",
    "            .rolling_mean(3, min_periods=1).over(\"token_address\")\n",
    "            .alias(\"avg_abs_change_3m\"),\n",
    "            \n",
    "            # Est-ce qu'on est en pump ? (>5% en 3 minutes)\n",
    "            (pl.col(\"price\") / pl.col(\"price\").shift(3).over(\"token_address\") > 1.05)\n",
    "            .fill_null(False).cast(pl.Int32)\n",
    "            .alias(\"is_pumping\"),\n",
    "            \n",
    "            # Est-ce qu'on est en dump ? (<-5% en 3 minutes)\n",
    "            (pl.col(\"price\") / pl.col(\"price\").shift(3).over(\"token_address\") < 0.95)\n",
    "            .fill_null(False).cast(pl.Int32)\n",
    "            .alias(\"is_dumping\"),\n",
    "        ])\n",
    "        return self.df\n",
    "    \n",
    "    def add_multi_step_targets(self, forecast_steps: int = 5) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Ajoute les targets pour prÃ©dire les N prochains points\n",
    "        forecast_steps: nombre de minutes Ã  prÃ©dire dans le futur\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Pour chaque step futur\n",
    "        for step in range(1, forecast_steps + 1):\n",
    "            df = df.with_columns([\n",
    "                # Prix futur Ã  chaque step\n",
    "                pl.col(\"price\").shift(-step).over(\"token_address\")\n",
    "                .alias(f\"target_price_t{step}\"),\n",
    "                \n",
    "                # Log prix (plus stable pour la prÃ©diction)\n",
    "                pl.col(\"log_price\").shift(-step).over(\"token_address\")\n",
    "                .alias(f\"target_log_price_t{step}\"),\n",
    "                \n",
    "                # Return cumulÃ© depuis maintenant jusqu'Ã  ce step\n",
    "                ((pl.col(\"price\").shift(-step) / pl.col(\"price\")) - 1)\n",
    "                .over(\"token_address\")\n",
    "                .alias(f\"target_return_t{step}\"),\n",
    "                \n",
    "                # Direction binaire Ã  chaque step\n",
    "                (pl.col(\"price\").shift(-step) > pl.col(\"price\"))\n",
    "                .over(\"token_address\")\n",
    "                .cast(pl.Int32)\n",
    "                .alias(f\"target_direction_t{step}\"),\n",
    "            ])\n",
    "        \n",
    "        # Ajouter des mÃ©triques agrÃ©gÃ©es sur la sÃ©quence future\n",
    "        df = df.with_columns([\n",
    "            # Prix max dans les N prochaines minutes\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_price_t{i}\") for i in range(1, forecast_steps + 1)\n",
    "            ]).list.max().alias(f\"target_max_price_next_{forecast_steps}m\"),\n",
    "            \n",
    "            # Prix min dans les N prochaines minutes\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_price_t{i}\") for i in range(1, forecast_steps + 1)\n",
    "            ]).list.min().alias(f\"target_min_price_next_{forecast_steps}m\"),\n",
    "            \n",
    "            # Y a-t-il un pump dans les N prochaines minutes? (>5%)\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_return_t{i}\") > 0.05 for i in range(1, forecast_steps + 1)\n",
    "            ]).list.any().cast(pl.Int32).alias(f\"target_has_pump_next_{forecast_steps}m\"),\n",
    "            \n",
    "            # Y a-t-il un dump dans les N prochaines minutes? (<-5%)\n",
    "            pl.concat_list([\n",
    "                pl.col(f\"target_return_t{i}\") < -0.05 for i in range(1, forecast_steps + 1)\n",
    "            ]).list.any().cast(pl.Int32).alias(f\"target_has_dump_next_{forecast_steps}m\"),\n",
    "        ])\n",
    "        \n",
    "        self.df = df\n",
    "        return self.df\n",
    "    \n",
    "    def create_all_features(self, forecast_steps: int = 5) -> pl.DataFrame:\n",
    "        \"\"\"Pipeline complet de feature engineering\"\"\"\n",
    "        print(\"1. Ajout des features de base...\")\n",
    "        self.add_core_features()\n",
    "        \n",
    "        print(\"2. Ajout des indicateurs techniques rapides...\")\n",
    "        self.add_fast_indicators()\n",
    "        \n",
    "        print(\"3. Ajout des features de dÃ©tection pump/dump...\")\n",
    "        self.add_pump_detection_features()\n",
    "        \n",
    "        print(\"4. Ajout des targets multi-step...\")\n",
    "        self.add_multi_step_targets(forecast_steps)\n",
    "        \n",
    "        print(f\"âœ… Feature engineering terminÃ©: {len(self.df.columns)} colonnes\")\n",
    "        return self.df\n",
    "\n",
    "\n",
    "def create_sequences_from_features(\n",
    "    df: pl.DataFrame,\n",
    "    sequence_length: int = 15,\n",
    "    forecast_steps: int = 5,\n",
    "    min_minutes_since_launch: int = 15\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    CrÃ©e des sÃ©quences Ã  partir du DataFrame avec features\n",
    "    Cette fonction est sÃ©parÃ©e de la classe pour plus de flexibilitÃ©\n",
    "    \"\"\"\n",
    "    \n",
    "    # Features pour l'input\n",
    "    feature_cols = [\n",
    "        \"minutes_since_launch\",\n",
    "        \"log_price\",\n",
    "        \"returns\",\n",
    "        \"price_multiple\",\n",
    "        \"price_to_ma5_ratio\",\n",
    "        \"volatility_5m\",\n",
    "        \"momentum_5m\",\n",
    "        \"is_pumping\",\n",
    "        \"is_dumping\",\n",
    "        \"avg_abs_change_3m\"\n",
    "    ]\n",
    "    \n",
    "    # Colonnes des targets\n",
    "    target_price_cols = [f\"target_log_price_t{i}\" for i in range(1, forecast_steps + 1)]\n",
    "    target_return_cols = [f\"target_return_t{i}\" for i in range(1, forecast_steps + 1)]\n",
    "    target_direction_cols = [f\"target_direction_t{i}\" for i in range(1, forecast_steps + 1)]\n",
    "    \n",
    "    # S'assurer qu'on a pas de NaN dans les colonnes critiques\n",
    "    check_cols = feature_cols + target_price_cols\n",
    "    df_clean = df.drop_nulls(subset=check_cols)\n",
    "    \n",
    "    sequences = []\n",
    "    target_sequences = []\n",
    "    target_returns = []\n",
    "    target_directions = []\n",
    "    metadata = []\n",
    "    \n",
    "    for token, group in df_clean.group_by(\"token_address\"):\n",
    "        group = group.sort(\"date_utc\")\n",
    "        \n",
    "        # Ne pas utiliser les toutes premiÃ¨res minutes\n",
    "        mask = group[\"minutes_since_launch\"] >= min_minutes_since_launch\n",
    "        group = group.filter(mask)\n",
    "        \n",
    "        if len(group) < sequence_length + forecast_steps:\n",
    "            continue\n",
    "        \n",
    "        # Extraire toutes les donnÃ©es nÃ©cessaires\n",
    "        features = group.select(feature_cols).to_numpy()\n",
    "        \n",
    "        # Targets : sÃ©quences futures complÃ¨tes\n",
    "        future_prices = group.select(target_price_cols).to_numpy()\n",
    "        future_returns = group.select(target_return_cols).to_numpy()\n",
    "        future_directions = group.select(target_direction_cols).to_numpy()\n",
    "        \n",
    "        # MÃ©triques agrÃ©gÃ©es\n",
    "        has_pump = group[f\"target_has_pump_next_{forecast_steps}m\"].to_numpy()\n",
    "        has_dump = group[f\"target_has_dump_next_{forecast_steps}m\"].to_numpy()\n",
    "        \n",
    "        # CrÃ©er des sÃ©quences\n",
    "        for i in range(sequence_length, len(features) - forecast_steps):\n",
    "            sequences.append(features[i-sequence_length:i])\n",
    "            target_sequences.append(future_prices[i])\n",
    "            target_returns.append(future_returns[i])\n",
    "            target_directions.append(future_directions[i])\n",
    "            \n",
    "            metadata.append({\n",
    "                \"token\": token[0],\n",
    "                \"timestamp\": group[\"date_utc\"][i],\n",
    "                \"minutes_since_launch\": int(group[\"minutes_since_launch\"][i]),\n",
    "                \"current_price\": float(group[\"price\"][i]),\n",
    "                \"has_pump_next\": int(has_pump[i]),\n",
    "                \"has_dump_next\": int(has_dump[i])\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"input_sequences\": np.array(sequences, dtype=np.float32),\n",
    "        \"target_sequences\": np.array(target_sequences, dtype=np.float32),\n",
    "        \"target_returns\": np.array(target_returns, dtype=np.float32),\n",
    "        \"target_directions\": np.array(target_directions, dtype=np.int32),\n",
    "        \"metadata\": metadata,\n",
    "        \"feature_names\": feature_cols,\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"forecast_steps\": forecast_steps,\n",
    "        \"n_features\": len(feature_cols)\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple d'utilisation\n",
    "    data_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/data_onchain_merged_tokens_list_high_score.parquet\")\n",
    "    df = pl.read_parquet(data_path)\n",
    "    \n",
    "    print(f\"DonnÃ©es chargÃ©es : {len(df)} lignes, {df['token_address'].n_unique()} tokens\")\n",
    "    \n",
    "    # Feature engineering\n",
    "    fe = MemecoinsFeatureEngineer(df)\n",
    "    df_features = fe.create_all_features(forecast_steps=5)\n",
    "    \n",
    "    # Sauvegarder le DataFrame avec features\n",
    "    output_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/memecoin_features_complete.parquet\")\n",
    "    df_features.write_parquet(output_path)\n",
    "    print(f\"\\nğŸ’¾ Features sauvegardÃ©es : {output_path}\")\n",
    "    \n",
    "    # CrÃ©er les sÃ©quences\n",
    "    print(\"\\nCrÃ©ation des sÃ©quences...\")\n",
    "    sequences_data = create_sequences_from_features(\n",
    "        df_features,\n",
    "        sequence_length=15,\n",
    "        forecast_steps=5,\n",
    "        min_minutes_since_launch=15\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… SÃ©quences crÃ©Ã©es :\")\n",
    "    print(f\"  - Nombre : {len(sequences_data['input_sequences'])}\")\n",
    "    print(f\"  - Shape input : {sequences_data['input_sequences'].shape}\")\n",
    "    print(f\"  - Shape target : {sequences_data['target_sequences'].shape}\")\n",
    "    \n",
    "    # Sauvegarder les sÃ©quences\n",
    "    sequences_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_raw.npz\")\n",
    "    np.savez_compressed(sequences_path, **sequences_data)\n",
    "    print(f\"ğŸ’¾ SÃ©quences sauvegardÃ©es : {sequences_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "822f1e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š DonnÃ©es originales: (4933860, 15, 10)\n",
      "ğŸ“Š Features: ['minutes_since_launch', 'log_price', 'returns', 'price_multiple', 'price_to_ma5_ratio', 'volatility_5m', 'momentum_5m', 'is_pumping', 'is_dumping', 'avg_abs_change_3m']\n",
      "âœ… Stats globales calculÃ©es pour 10 features\n",
      "ğŸ”„ Scaling des sÃ©quences...\n",
      "ğŸ”„ Scaling des sÃ©quences (version rapide)...\n",
      "âœ… Scaling terminÃ©!\n",
      "ğŸ“Š Split: 2783 tokens train, 695 tokens validation\n",
      "\n",
      "âœ… DonnÃ©es prÃ©parÃ©es:\n",
      "  - Train: 3949720 sÃ©quences\n",
      "  - Validation: 984140 sÃ©quences\n",
      "  - Shape input: (3949720, 15, 10)\n",
      "  - Shape target: (3949720, 5)\n",
      "\n",
      "ğŸ“Š Statistiques aprÃ¨s scaling:\n",
      "  - minutes_since_launch: mean=0.365, std=0.212\n",
      "  - log_price: mean=-0.006, std=0.421\n",
      "  - returns: mean=29442648.000, std=69870747648.000\n",
      "  - price_multiple: mean=3.510, std=7.210\n",
      "  - price_to_ma5_ratio: mean=-0.002, std=0.120\n",
      "\n",
      "ğŸ’¾ DonnÃ©es scalÃ©es sauvegardÃ©es: /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_scaled.npz\n",
      "ğŸ’¾ Metadata sauvegardÃ©s: /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_metadata.json\n",
      "ğŸ’¾ ParamÃ¨tres du scaler sauvegardÃ©s: /Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/scaler_params.json\n",
      "\n",
      "ğŸ“‹ Exemple de donnÃ©es:\n",
      "  - Input shape: (15, 10)\n",
      "  - Target shape: (5,)\n",
      "  - Scaling params: ['last_log_price']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data_scaling.py - Version corrigÃ©e\n",
    "Scaling et prÃ©paration des donnÃ©es pour l'entraÃ®nement\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import json\n",
    "\n",
    "class MemecoinSequenceScaler:\n",
    "    \"\"\"\n",
    "    Scaler spÃ©cialisÃ© pour les sÃ©quences de trading de memecoins\n",
    "    Applique diffÃ©rentes stratÃ©gies de scaling selon le type de feature\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_method: str = \"robust\"):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.feature_names = None\n",
    "        self.global_stats = {}\n",
    "        self.feature_groups = {\n",
    "            \"price_features\": [\"log_price\"],\n",
    "            \"return_features\": [\"returns\", \"log_returns\", \"volatility_5m\", \"avg_abs_change_3m\"],\n",
    "            \"ratio_features\": [\"price_multiple\", \"price_to_ma5_ratio\", \"momentum_5m\", \"price_range_5m\"],\n",
    "            \"temporal_features\": [\"minutes_since_launch\", \"hour_of_day\", \"minute_of_hour\"],\n",
    "            \"binary_features\": [\"is_pumping\", \"is_dumping\", \"consecutive_ups_3m\"]\n",
    "        }\n",
    "        \n",
    "    def fit_global_stats(self, all_sequences: np.ndarray, feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Calcule les statistiques globales pour certaines features\n",
    "        \"\"\"\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Pour chaque type de feature, calculer les stats appropriÃ©es\n",
    "        for feat_type, feat_list in self.feature_groups.items():\n",
    "            for feat_name in feat_list:\n",
    "                if feat_name in feature_names:\n",
    "                    idx = feature_names.index(feat_name)\n",
    "                    all_values = all_sequences[:, :, idx].flatten()\n",
    "                    \n",
    "                    # Filtrer les NaN\n",
    "                    valid_values = all_values[~np.isnan(all_values)]\n",
    "                    \n",
    "                    if len(valid_values) > 0:\n",
    "                        self.global_stats[feat_name] = {\n",
    "                            \"min\": float(np.min(valid_values)),\n",
    "                            \"max\": float(np.max(valid_values)),\n",
    "                            \"mean\": float(np.mean(valid_values)),\n",
    "                            \"std\": float(np.std(valid_values)),\n",
    "                            \"median\": float(np.median(valid_values)),\n",
    "                            \"q1\": float(np.percentile(valid_values, 25)),\n",
    "                            \"q3\": float(np.percentile(valid_values, 75))\n",
    "                        }\n",
    "        \n",
    "        print(f\"âœ… Stats globales calculÃ©es pour {len(self.global_stats)} features\")\n",
    "    \n",
    "    def scale_sequences(\n",
    "        self, \n",
    "        sequences: np.ndarray, \n",
    "        targets: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Scale toutes les sÃ©quences en une fois (vectorisÃ©)\n",
    "        \"\"\"\n",
    "        n_samples, seq_length, n_features = sequences.shape\n",
    "        scaled_sequences = np.zeros_like(sequences)\n",
    "        scaled_targets = np.zeros_like(targets)\n",
    "        \n",
    "        # 1. Prix - Scaling par rapport au dernier point\n",
    "        if \"log_price\" in self.feature_names:\n",
    "            idx = self.feature_names.index(\"log_price\")\n",
    "            # Extraire tous les derniers prix en une fois\n",
    "            last_prices = sequences[:, -1, idx]  # Shape: (n_samples,)\n",
    "            # Broadcaster pour soustraire de chaque timestep\n",
    "            scaled_sequences[:, :, idx] = sequences[:, :, idx] - last_prices[:, np.newaxis]\n",
    "            # Scaler les targets aussi\n",
    "            scaled_targets = targets - last_prices[:, np.newaxis]\n",
    "        \n",
    "        # 2. Returns et volatilitÃ© - Calcul vectorisÃ© des stats\n",
    "        for feat_name in self.feature_groups[\"return_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                feat_data = sequences[:, :, idx]  # Shape: (n_samples, seq_length)\n",
    "                \n",
    "                if self.scaling_method == \"robust\":\n",
    "                    # Calcul vectorisÃ© des percentiles par sÃ©quence\n",
    "                    q1 = np.nanpercentile(feat_data, 25, axis=1, keepdims=True)\n",
    "                    q3 = np.nanpercentile(feat_data, 75, axis=1, keepdims=True)\n",
    "                    median = np.nanmedian(feat_data, axis=1, keepdims=True)\n",
    "                    iqr = q3 - q1\n",
    "                    \n",
    "                    # Ã‰viter division par zÃ©ro\n",
    "                    iqr = np.where(iqr > 1e-8, iqr, self.global_stats.get(feat_name, {}).get(\"std\", 1.0))\n",
    "                    \n",
    "                    scaled_sequences[:, :, idx] = (feat_data - median) / iqr\n",
    "                else:\n",
    "                    # Standard scaling vectorisÃ©\n",
    "                    mean = np.nanmean(feat_data, axis=1, keepdims=True)\n",
    "                    std = np.nanstd(feat_data, axis=1, keepdims=True)\n",
    "                    std = np.where(std > 1e-8, std, 1.0)\n",
    "                    scaled_sequences[:, :, idx] = (feat_data - mean) / std\n",
    "        \n",
    "        # 3. Ratios - Log transform vectorisÃ©\n",
    "        for feat_name in self.feature_groups[\"ratio_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                scaled_sequences[:, :, idx] = np.log(sequences[:, :, idx] + 1e-8)\n",
    "        \n",
    "        # 4. Features temporelles - Normalisation globale vectorisÃ©e\n",
    "        for feat_name in self.feature_groups[\"temporal_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                if feat_name == \"minutes_since_launch\" and feat_name in self.global_stats:\n",
    "                    stats = self.global_stats[feat_name]\n",
    "                    scaled_sequences[:, :, idx] = (sequences[:, :, idx] - stats[\"min\"]) / (stats[\"max\"] - stats[\"min\"] + 1e-8)\n",
    "                elif feat_name == \"hour_of_day\":\n",
    "                    scaled_sequences[:, :, idx] = sequences[:, :, idx] / 24.0\n",
    "                elif feat_name == \"minute_of_hour\":\n",
    "                    scaled_sequences[:, :, idx] = sequences[:, :, idx] / 60.0\n",
    "        \n",
    "        # 5. Features binaires - Copie directe\n",
    "        for feat_name in self.feature_groups[\"binary_features\"]:\n",
    "            if feat_name in self.feature_names:\n",
    "                idx = self.feature_names.index(feat_name)\n",
    "                scaled_sequences[:, :, idx] = sequences[:, :, idx]\n",
    "        \n",
    "        # Remplacer NaN\n",
    "        scaled_sequences = np.nan_to_num(scaled_sequences, nan=0.0)\n",
    "        scaled_targets = np.nan_to_num(scaled_targets, nan=0.0)\n",
    "        \n",
    "        # CrÃ©er les params (simplifiÃ© pour la version batch)\n",
    "        scaling_params = [{\n",
    "            \"last_log_price\": float(last_prices[i]) if \"log_price\" in self.feature_names else 0.0\n",
    "        } for i in range(n_samples)]\n",
    "        \n",
    "        return scaled_sequences, scaled_targets, scaling_params\n",
    "    \n",
    "    def inverse_scale_predictions(\n",
    "        self, \n",
    "        predictions: np.ndarray, \n",
    "        scaling_params: Dict\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Inverse le scaling pour les prÃ©dictions\n",
    "        \"\"\"\n",
    "        if \"last_log_price\" in scaling_params:\n",
    "            # Les prÃ©dictions sont des diffÃ©rences par rapport au dernier log_price\n",
    "            return predictions + scaling_params[\"last_log_price\"]\n",
    "        return predictions\n",
    "    \n",
    "    def save_scaler(self, path: Path):\n",
    "        \"\"\"Sauvegarde les paramÃ¨tres du scaler\"\"\"\n",
    "        scaler_data = {\n",
    "            \"scaling_method\": self.scaling_method,\n",
    "            \"feature_names\": self.feature_names,\n",
    "            \"global_stats\": self.global_stats,\n",
    "            \"feature_groups\": self.feature_groups\n",
    "        }\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(scaler_data, f, indent=2)\n",
    "    \n",
    "    def load_scaler(self, path: Path):\n",
    "        \"\"\"Charge les paramÃ¨tres du scaler\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            scaler_data = json.load(f)\n",
    "        self.scaling_method = scaler_data[\"scaling_method\"]\n",
    "        self.feature_names = scaler_data[\"feature_names\"]\n",
    "        self.global_stats = scaler_data[\"global_stats\"]\n",
    "        self.feature_groups = scaler_data[\"feature_groups\"]\n",
    "\n",
    "\n",
    "class DataPreparer:\n",
    "    \"\"\"\n",
    "    PrÃ©pare les donnÃ©es pour l'entraÃ®nement avec train/val split et scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences_path: Path):\n",
    "        # Charger les sÃ©quences avec allow_pickle=True pour les metadata\n",
    "        data = np.load(sequences_path, allow_pickle=True)\n",
    "        \n",
    "        # Convertir en dictionnaire normal\n",
    "        self.sequences_data = {}\n",
    "        for key in data.files:\n",
    "            self.sequences_data[key] = data[key]\n",
    "            \n",
    "        # Convertir les types si nÃ©cessaire\n",
    "        if \"feature_names\" in self.sequences_data and isinstance(self.sequences_data[\"feature_names\"], np.ndarray):\n",
    "            if self.sequences_data[\"feature_names\"].ndim == 0:\n",
    "                # Cas oÃ¹ c'est un scalar array contenant une liste\n",
    "                self.sequences_data[\"feature_names\"] = self.sequences_data[\"feature_names\"].item()\n",
    "            else:\n",
    "                self.sequences_data[\"feature_names\"] = self.sequences_data[\"feature_names\"].tolist()\n",
    "                \n",
    "        # Convertir les autres champs scalaires si nÃ©cessaire\n",
    "        for key in [\"sequence_length\", \"forecast_steps\", \"n_features\"]:\n",
    "            if key in self.sequences_data and isinstance(self.sequences_data[key], np.ndarray):\n",
    "                self.sequences_data[key] = int(self.sequences_data[key].item())\n",
    "        \n",
    "        self.scaler = MemecoinSequenceScaler(scaling_method=\"robust\")\n",
    "        \n",
    "    def prepare_for_training(\n",
    "        self, \n",
    "        validation_split: float = 0.2,\n",
    "        random_seed: int = 42\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        PrÃ©pare les donnÃ©es complÃ¨tes pour l'entraÃ®nement\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        # Extraire les donnÃ©es\n",
    "        input_sequences = self.sequences_data[\"input_sequences\"]\n",
    "        target_sequences = self.sequences_data[\"target_sequences\"]\n",
    "        metadata = self.sequences_data[\"metadata\"]\n",
    "        \n",
    "        # GÃ©rer les feature_names selon le format\n",
    "        if isinstance(self.sequences_data[\"feature_names\"], list):\n",
    "            feature_names = self.sequences_data[\"feature_names\"]\n",
    "        else:\n",
    "            feature_names = self.sequences_data[\"feature_names\"].tolist()\n",
    "        \n",
    "        print(f\"ğŸ“Š DonnÃ©es originales: {input_sequences.shape}\")\n",
    "        print(f\"ğŸ“Š Features: {feature_names}\")\n",
    "        \n",
    "        # Fit le scaler sur toutes les donnÃ©es\n",
    "        self.scaler.fit_global_stats(input_sequences, feature_names)\n",
    "        \n",
    "        # Scaler toutes les sÃ©quences\n",
    "        print(\"ğŸ”„ Scaling des sÃ©quences...\")\n",
    "        scaled_inputs = []\n",
    "        scaled_targets = []\n",
    "        all_scaling_params = []\n",
    "        \n",
    "        # for i in range(len(input_sequences)):\n",
    "        #     scaled_input, scaled_target, params = self.scaler.scale_sequence(\n",
    "        #         input_sequences[i], \n",
    "        #         target_sequences[i]\n",
    "        #     )\n",
    "        #     scaled_inputs.append(scaled_input)\n",
    "        #     scaled_targets.append(scaled_target)\n",
    "        #     all_scaling_params.append(params)\n",
    "\n",
    "        print(\"ğŸ”„ Scaling des sÃ©quences (version rapide)...\")\n",
    "        scaled_inputs, scaled_targets, all_scaling_params = self.scaler.scale_sequences(\n",
    "            input_sequences, \n",
    "            target_sequences\n",
    "        )\n",
    "        print(\"âœ… Scaling terminÃ©!\")\n",
    "\n",
    "        scaled_inputs = np.array(scaled_inputs)\n",
    "        scaled_targets = np.array(scaled_targets)\n",
    "        \n",
    "        # Split par token pour Ã©viter le data leakage\n",
    "        unique_tokens = list(set([m[\"token\"] for m in metadata]))\n",
    "        np.random.shuffle(unique_tokens)\n",
    "        \n",
    "        n_val_tokens = int(len(unique_tokens) * validation_split)\n",
    "        val_tokens = set(unique_tokens[:n_val_tokens])\n",
    "        \n",
    "        print(f\"ğŸ“Š Split: {len(unique_tokens)-n_val_tokens} tokens train, {n_val_tokens} tokens validation\")\n",
    "        \n",
    "        # CrÃ©er les indices\n",
    "        train_idx = [i for i, m in enumerate(metadata) if m[\"token\"] not in val_tokens]\n",
    "        val_idx = [i for i, m in enumerate(metadata) if m[\"token\"] in val_tokens]\n",
    "        \n",
    "        # CrÃ©er les datasets\n",
    "        train_data = {\n",
    "            \"inputs\": scaled_inputs[train_idx],\n",
    "            \"targets\": scaled_targets[train_idx],\n",
    "            \"metadata\": [metadata[i] for i in train_idx],\n",
    "            \"scaling_params\": [all_scaling_params[i] for i in train_idx]\n",
    "        }\n",
    "        \n",
    "        val_data = {\n",
    "            \"inputs\": scaled_inputs[val_idx],\n",
    "            \"targets\": scaled_targets[val_idx],\n",
    "            \"metadata\": [metadata[i] for i in val_idx],\n",
    "            \"scaling_params\": [all_scaling_params[i] for i in val_idx]\n",
    "        }\n",
    "        \n",
    "        # Statistiques\n",
    "        print(f\"\\nâœ… DonnÃ©es prÃ©parÃ©es:\")\n",
    "        print(f\"  - Train: {len(train_data['inputs'])} sÃ©quences\")\n",
    "        print(f\"  - Validation: {len(val_data['inputs'])} sÃ©quences\")\n",
    "        print(f\"  - Shape input: {train_data['inputs'].shape}\")\n",
    "        print(f\"  - Shape target: {train_data['targets'].shape}\")\n",
    "        \n",
    "        # VÃ©rifier les distributions\n",
    "        print(f\"\\nğŸ“Š Statistiques aprÃ¨s scaling:\")\n",
    "        for i, feat in enumerate(feature_names[:5]):  # Top 5 features\n",
    "            train_values = train_data[\"inputs\"][:, :, i].flatten()\n",
    "            print(f\"  - {feat}: mean={np.mean(train_values):.3f}, std={np.std(train_values):.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"train\": train_data,\n",
    "            \"validation\": val_data,\n",
    "            \"scaler\": self.scaler,\n",
    "            \"feature_names\": feature_names,\n",
    "            \"metadata\": {\n",
    "                \"sequence_length\": self.sequences_data.get(\"sequence_length\", 15),\n",
    "                \"forecast_steps\": self.sequences_data.get(\"forecast_steps\", 5),\n",
    "                \"n_features\": self.sequences_data.get(\"n_features\", len(feature_names))\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Chemins\n",
    "    sequences_path = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/sequences_raw.npz\")\n",
    "    output_dir = Path(\"/Users/stordd/Documents/GitHub/Solana/memecoin2/data/jeff/\")\n",
    "    \n",
    "    # PrÃ©parer les donnÃ©es\n",
    "    preparer = DataPreparer(sequences_path)\n",
    "    prepared_data = preparer.prepare_for_training(validation_split=0.2)\n",
    "    \n",
    "    # Sauvegarder les donnÃ©es scalÃ©es - SANS les metadata objects\n",
    "    output_path = output_dir / \"sequences_scaled.npz\"\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        train_inputs=prepared_data[\"train\"][\"inputs\"],\n",
    "        train_targets=prepared_data[\"train\"][\"targets\"],\n",
    "        val_inputs=prepared_data[\"validation\"][\"inputs\"],\n",
    "        val_targets=prepared_data[\"validation\"][\"targets\"],\n",
    "        feature_names=prepared_data[\"feature_names\"],\n",
    "        **prepared_data[\"metadata\"]\n",
    "    )\n",
    "    print(f\"\\nğŸ’¾ DonnÃ©es scalÃ©es sauvegardÃ©es: {output_path}\")\n",
    "    \n",
    "    # Sauvegarder les metadata sÃ©parÃ©ment en JSON\n",
    "    metadata_path = output_dir / \"sequences_metadata.json\"\n",
    "    metadata_to_save = {\n",
    "        \"train_tokens\": list(set([m[\"token\"] for m in prepared_data[\"train\"][\"metadata\"]])),\n",
    "        \"val_tokens\": list(set([m[\"token\"] for m in prepared_data[\"validation\"][\"metadata\"]])),\n",
    "        \"n_train_sequences\": len(prepared_data[\"train\"][\"inputs\"]),\n",
    "        \"n_val_sequences\": len(prepared_data[\"validation\"][\"inputs\"]),\n",
    "        \"feature_names\": prepared_data[\"feature_names\"],\n",
    "        **prepared_data[\"metadata\"]\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata_to_save, f, indent=2)\n",
    "    print(f\"ğŸ’¾ Metadata sauvegardÃ©s: {metadata_path}\")\n",
    "    \n",
    "    # Sauvegarder le scaler\n",
    "    scaler_path = output_dir / \"scaler_params.json\"\n",
    "    prepared_data[\"scaler\"].save_scaler(scaler_path)\n",
    "    print(f\"ğŸ’¾ ParamÃ¨tres du scaler sauvegardÃ©s: {scaler_path}\")\n",
    "    \n",
    "    # Exemple d'utilisation\n",
    "    print(\"\\nğŸ“‹ Exemple de donnÃ©es:\")\n",
    "    example_idx = 0\n",
    "    print(f\"  - Input shape: {prepared_data['train']['inputs'][example_idx].shape}\")\n",
    "    print(f\"  - Target shape: {prepared_data['train']['targets'][example_idx].shape}\")\n",
    "    print(f\"  - Scaling params: {list(prepared_data['train']['scaling_params'][example_idx].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61241cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5984cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
