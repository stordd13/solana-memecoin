# M√©thodologie Technique Data Science - Pipeline d'Analyse de Memecoins

## 1. Approche d'Analyse et de Cat√©gorisation des Tokens

### 1.1 Syst√®me de Scoring de Qualit√© des Donn√©es
Nous avons d√©velopp√© un syst√®me de scoring multi-dimensionnel qui analyse chaque token selon plusieurs m√©triques :

```python
quality_score = 100 - gap_penalty - anomaly_penalty - volatility_penalty
```

**M√©triques Cl√©s :**
- **Couverture Temporelle** : Minutes depuis le lancement, analyse des gaps (>1 min entre points cons√©cutifs)
- **Anomalies de Prix** : Prix z√©ro, prix n√©gatifs, sauts extr√™mes
- **Compl√©tude des Donn√©es** : Valeurs manquantes, timestamps dupliqu√©s
- **Mouvements Extr√™mes** : Changements de prix utilisant des seuils sophistiqu√©s

### 1.2 D√©tection des Mouvements Extr√™mes (Seuils Mis √† Jour)

**Syst√®me de D√©tection Sophistiqu√© :**
```python
# Seuils actuels dans data_analysis/data_quality.py
extreme_thresholds = {
    'extreme_minute_return': 100.0,    # 10 000% en une minute
    'extreme_total_return': 10000.0,   # 1 000 000% de rendement total
    'extreme_volatility': 100.0,       # 10 000% de volatilit√©  
    'extreme_range': 100.0             # 10 000% de fourchette de prix
}

# Logique de d√©tection
extreme_minute_mask = pl.col('returns') > 100.0  # 10 000% en une minute
total_return = ((last_price - first_price) / first_price) * 100
has_extreme_total = abs(total_return) > 1000000  # 1M% total

is_extreme_token = (
    has_extreme_minute_jump or 
    has_extreme_total_return or
    has_extreme_volatility or 
    has_extreme_return or 
    has_extreme_range
)
```

**Pourquoi Ces Seuils √âlev√©s :**
- **10 000% (100x) de pumps en une minute** : Absolument possible sur les march√©s de memecoins
- **1 000 000% (10 000x)** : Seuil clair de corruption de donn√©es
- **Pr√©serve les comportements extr√™mes l√©gitimes** tout en filtrant les erreurs √©videntes

### 1.3 Logique de Cat√©gorisation des Tokens

Nous utilisons une **hi√©rarchie mutuellement exclusive** pour s'assurer que chaque token appara√Æt dans exactement UNE cat√©gorie :

```
Priorit√© : gaps > normal > extremes > dead
```

**Distribution Actuelle (apr√®s analyse) :**
1. **tokens_with_gaps** : ~22 tokens (priorit√© la plus √©lev√©e, exclus de l'entra√Ænement)
2. **normal_behavior_tokens** : ~3 426 tokens (qualit√© premium)
3. **tokens_with_extremes** : ~1 802 tokens (pumps/dumps l√©gitimes)
4. **dead_tokens** : ~23 567 tokens (cycles de vie complets)

**Logique d'Attribution des Cat√©gories :**
```python
# 1. V√©rifier d'abord les gaps (priorit√© la plus √©lev√©e)
if max_gap_minutes > 10:
    category = 'tokens_with_gaps'
    
# 2. V√©rifier le score de qualit√© et les extr√™mes
elif quality_score >= 80 and not has_extreme_movements:
    category = 'normal_behavior_tokens'
    
# 3. A des mouvements extr√™mes (l√©gitimes)
elif has_extreme_movements:
    category = 'tokens_with_extremes'
    
# 4. Par d√©faut vers les tokens morts
else:
    category = 'dead_tokens'
```

## 2. Strat√©gies de Nettoyage des Donn√©es (Cat√©gorie-Aware)

### 2.1 Mapping des Strat√©gies
Chaque cat√©gorie re√ßoit un nettoyage sur mesure pour pr√©server ses caract√©ristiques d√©finissantes :

```python
CATEGORIES = {
    'normal_behavior_tokens': 'gentle',     # Pr√©server la volatilit√© naturelle
    'dead_tokens': 'minimal',               # Supprimer les p√©riodes constantes (anti-leakage)
    'tokens_with_extremes': 'preserve',     # Garder TOUS les mouvements extr√™mes
    'tokens_with_gaps': 'aggressive'        # Combler les gaps agressivement
}
```

### 2.2 Seuils et D√©tection de Nettoyage

**Seuils d'Artefacts vs. Mouvements L√©gitimes :**
```python
# data_cleaning/clean_tokens.py
artifact_thresholds = {
    'listing_spike_multiplier': 20,     # 20x m√©diane pour artefacts de listing
    'listing_drop_threshold': 0.99,     # 99% de chute apr√®s pic
    'data_error_threshold': 1000,       # 100 000% (erreurs de donn√©es √©videntes)
    'flash_crash_recovery': 0.95,       # 95% de r√©cup√©ration en 5 minutes
}

# Seuils de comportement de march√© (PR√âSERVER ceux-ci !)
market_thresholds = {
    'max_realistic_pump': 50,           # 5 000% de pumps sont r√©els
    'max_realistic_dump': 0.95,         # 95% de dumps sont r√©els
    'sustained_movement_minutes': 3,    # Les vrais mouvements durent >3 minutes
}
```

### 2.3 Nettoyage de Pr√©servation des Extr√™mes (Le Plus Conservateur)

**Pour tokens_with_extremes - garde TOUS les mouvements l√©gitimes :**
```python
def _preserve_extremes_cleaning(self, df, token_name):
    """
    SUPPRIMER SEULEMENT la corruption √©vidente des donn√©es :
    - Valeurs impossibles (prix n√©gatifs, z√©ros exacts)
    - Corruption extr√™me des donn√©es (>1 000 000% en une minute)
    - Gaps critiques qui cassent la continuit√©
    
    PR√âSERVE :
    - 10 000% (100x) mouvements par minute - L√âGITIME
    - 5 000% (50x) pumps soutenus - L√âGITIME  
    - 95% dumps - L√âGITIME
    """
    # Supprimer seulement les valeurs impossibles
    df = self._handle_impossible_values_only(df)
    
    # Corriger seulement la corruption >1M% (pas les mouvements l√©gitimes 10K%)
    df = self._fix_extreme_data_corruption(df)
    
    # Combler seulement les gaps critiques
    df = self._fill_critical_gaps_only(df)
```

### 2.4 Nettoyage Anti-Leakage des Tokens Morts

**Critique pour pr√©venir le leakage de donn√©es dans les tokens morts :**
```python
def _minimal_cleaning(self, df, token_name):
    """
    Pour dead_tokens : Supprimer les p√©riodes de prix constants pour pr√©venir le leakage
    """
    # D√©tecter les p√©riodes de prix constants depuis la fin
    constant_count = 0
    for i in range(len(df)-1, 0, -1):
        if abs(prices[i] - prices[i-1]) < 1e-10:
            constant_count += 1
        else:
            break
    
    # Supprimer la p√©riode constante mais garder 2 minutes pour le contexte
    if constant_count >= 60:  # 1+ heure constante
        remove_count = constant_count - 2
        df_cleaned = df[:-remove_count]
        
    print(f"üõ°Ô∏è ANTI-LEAKAGE: {token_name} - Supprim√© {remove_count} minutes")
```

## 3. Approche d'Ing√©nierie des Features

### 3.1 D√©cision d'Architecture : Features Pr√©-Ing√©nieur√©es
Nous calculons les features UNE FOIS et les stockons dans `data/features/`, plut√¥t que de les calculer √† la vol√©e :

**Avantages :**
- **Consistance** : M√™mes features √† travers tous les 8 mod√®les
- **Performance** : 10x plus rapide d'exp√©rimentation
- **Debugging** : Validation plus facile et d√©tection de leakage
- **Modularit√©** : S√©paration propre du pipeline

### 3.2 Features Rolling ML-Safe Seulement

**Features Stock√©es** (dans `data/features/`) :
```python
# feature_engineering/advanced_feature_engineering.py

# Features bas√©es sur le prix (fondamentales)
log_returns = np.log(prices[1:] / prices[:-1])
price_momentum = prices / prices.shift(periods)

# Statistiques rolling (fen√™tres expanding pour pr√©venir le leakage)
rolling_volatility = log_returns.expanding().std()
rolling_sharpe = log_returns.expanding().mean() / log_returns.expanding().std()
rolling_skewness = log_returns.expanding().skew()
rolling_kurtosis = log_returns.expanding().kurt()

# Indicateurs techniques (tous r√©trospectifs)
rsi = calculate_rsi(prices, window=14)
macd = ema_12 - ema_26
bollinger_position = (price - sma_20) / (2 * rolling_std_20)
atr = calculate_atr(high, low, close, window=14)

# Features avanc√©es
fft_dominant_freq = np.fft.fft(log_returns[-min(len(log_returns), 60):])
correlation_with_market = rolling_correlation(token_returns, market_returns)

# Bas√©es sur le volume (si disponible)
volume_sma = volume.rolling(window=20).mean()
price_volume_trend = calculate_pvt(prices, volume)
```

**Winsorisation Appliqu√©e :**
```python
# ML/utils/winsorizer.py
winsorizer = Winsorizer(
    lower_quantile=0.005,  # 0.5√®me percentile
    upper_quantile=0.995   # 99.5√®me percentile
)
# G√®re les outliers extr√™mes de crypto sans perdre l'information relative
```

**Features Globales** (calcul√©es √† la demande dans Streamlit - NON stock√©es √† cause du leakage) :
- Rendement total % (utilise le prix final - LEAKAGE)
- Drawdown maximum (utilise le minimum futur - LEAKAGE)  
- Classification de motifs (utilise la s√©rie compl√®te - LEAKAGE)

### 3.3 Ing√©nierie de Lookback Variable
```python
# Commencer les pr√©dictions √† la minute 3 avec les donn√©es disponibles
for minute in range(3, token_length):
    lookback_data = data[:minute]  # Utiliser seulement les donn√©es pass√©es
    
    features = {
        'rsi_14': calculate_rsi(lookback_data) if minute >= 15 else np.nan,
        'rolling_vol_10': lookback_data[-10:].std() if minute >= 10 else np.nan,
        'price_momentum_5': current_price / lookback_data[-5] if minute >= 5 else np.nan,
    }
```

## 4. M√©thodologie de Scaling des Donn√©es

### 4.1 Scaling Par-Token (D√©cision Critique)
Chaque token a des fourchettes de prix vastement diff√©rentes (de $0.0000001 √† $100+), donc nous scalons PAR TOKEN :

```python
# Pour chaque token individuellement :
def scale_token_data(token_data):
    # Diviser temporellement EN PREMIER
    split_idx_train = int(0.6 * len(token_data))
    split_idx_val = int(0.8 * len(token_data))
    
    train_data = token_data[:split_idx_train]
    val_data = token_data[split_idx_train:split_idx_val]
    test_data = token_data[split_idx_val:]
    
    # Ajuster le scaler SEULEMENT sur les donn√©es d'entra√Ænement
    scaler = Winsorizer(lower_quantile=0.005, upper_quantile=0.995)
    scaler.fit(train_data)
    
    # Appliquer √† tous les splits
    train_scaled = scaler.transform(train_data)
    val_scaled = scaler.transform(val_data)
    test_scaled = scaler.transform(test_data)
    
    return train_scaled, val_scaled, test_scaled, scaler
```

### 4.2 M√©thodes de Scaling par Mod√®le

**Winsorizer** (Pr√©f√©r√© pour crypto) :
```python
# Plafonne aux percentiles extr√™mes, pr√©serve la forme de distribution
winsorizer = Winsorizer(lower_quantile=0.005, upper_quantile=0.995)
# Utilis√© dans : R√©gression Logistique, Tous les mod√®les LSTM, Ing√©nierie de features
```

**RobustScaler** (Sauvegarde) :
```python
# Utilise la m√©diane et l'IQR (r√©sistant aux outliers)
# Utilis√© dans : Certains mod√®les de base quand winsorizer non disponible
```

### 4.3 Correction IQR Z√©ro (Critique pour les Tokens Morts)
```python
# G√©rer les features constantes dans les tokens morts
def fix_zero_variance(scaler, feature_matrix):
    zero_variance_mask = np.isclose(scaler.scale_, 0)
    if np.any(zero_variance_mask):
        scaler.scale_[zero_variance_mask] = 1.0  # Pr√©venir la division par z√©ro
        print(f"Corrig√© {np.sum(zero_variance_mask)} features √† variance nulle")
```

## 5. Validation Walk-Forward (Enti√®rement Impl√©ment√©e)

### 5.1 Statut d'Impl√©mentation
**TOUS les 8 mod√®les utilisent maintenant la validation walk-forward :**
- ‚úÖ LightGBM Directionnel (`train_lightgbm_model.py`)
- ‚úÖ LightGBM Moyen-terme (`train_lightgbm_model_medium_term.py`)
- ‚úÖ LSTM Unifi√© (`train_unified_lstm_model.py`)
- ‚úÖ LSTM Hybride Avanc√© (`train_advanced_hybrid_lstm.py`)
- ‚úÖ LSTM Forecasting (`train_lstm_model.py`)
- ‚úÖ Forecasting Hybride Avanc√© (`train_advanced_hybrid_lstm_forecasting.py`)
- ‚úÖ R√©gresseurs de Base (`train_baseline_regressors.py`)
- ‚úÖ R√©gression Logistique (`train_logistic_regression_baseline.py`)

### 5.2 Configuration WalkForwardSplitter

**Configuration Adaptative par Longueur de Token :**
```python
# ML/utils/walk_forward_splitter.py
class WalkForwardSplitter:
    def __init__(self, config='medium'):
        if config == 'short':      # tokens de 400-600 minutes
            self.min_train_size = 240    # 4 heures minimum
            self.step_size = 60          # 1 heure d'√©tapes forward
            self.test_size = 60          # 1 heure de fen√™tres de test
        elif config == 'medium':   # tokens de 600-1500 minutes
            self.min_train_size = 480    # 8 heures minimum
            self.step_size = 120         # 2 heures d'√©tapes forward  
            self.test_size = 120         # 2 heures de fen√™tres de test
        elif config == 'long':     # tokens de 1500+ minutes
            self.min_train_size = 960    # 16 heures minimum
            self.step_size = 240         # 4 heures d'√©tapes forward
            self.test_size = 240         # 4 heures de fen√™tres de test
```

**Exemple Walk-Forward :**
```
Token avec 1380 minutes (typique apr√®s nettoyage) :
Utilisant la config 'medium' :

Fold 1: Train[0:480]   ‚Üí Test[480:600]   (120 min test)
Fold 2: Train[0:600]   ‚Üí Test[600:720]   (120 min test)  
Fold 3: Train[0:720]   ‚Üí Test[720:840]   (120 min test)
Fold 4: Train[0:840]   ‚Üí Test[840:960]   (120 min test)
Fold 5: Train[0:960]   ‚Üí Test[960:1080]  (120 min test)
Fold 6: Train[0:1080]  ‚Üí Test[1080:1200] (120 min test)
Final:  Train[0:1200]  ‚Üí Test[1200:1380] (180 min test)

R√©sultats : 7 folds de validation, m√©triques moyenn√©es sur tous les folds
```

### 5.3 Deux Strat√©gies Walk-Forward

**Splits Par-Token** (mod√®les LSTM) :
```python
# Meilleur pour les mod√®les de s√©quence qui apprennent des motifs sp√©cifiques aux tokens
token_splits = splitter.split_by_token(
    combined_data, 
    token_column='token_id',
    time_column='datetime',
    min_token_length=400
)
```

**Splits Globaux** (mod√®les LightGBM) :
```python
# Meilleur pour les mod√®les d'arbres qui apprennent des motifs inter-tokens
global_splits = splitter.get_global_splits(
    combined_data, 
    time_column='datetime'
)
```

## 6. Approches d'Entra√Ænement des Mod√®les (Architectures Mises √† Jour)

### 6.1 Mod√®les Directionnels (Classification)

**T√¢che** : Pr√©dire le mouvement HAUT/BAS √† horizons multiples (15m, 30m, 1h, 2h, 4h, 6h, 12h, 24h)

**1. Mod√®les LightGBM**
```python
# Horizons court-terme : 15m, 30m, 1h, 2h, 4h, 6h, 12h
# Horizons moyen-terme : 2h, 4h, 6h, 8h, 12h, 16h, 24h

params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 64,
    'learning_rate': 0.05,      # Court-terme
    'learning_rate': 0.03,      # Moyen-terme (plus stable)
    'feature_fraction': 0.8,    # Court-terme
    'feature_fraction': 0.7,    # Moyen-terme (plus de r√©gularisation)
    'bagging_fraction': 0.8,
    'min_child_samples': 20,
    'reg_alpha': 0.1,           # R√©gularisation L1  
    'reg_lambda': 0.1,          # R√©gularisation L2
}
```

**2. LSTM Unifi√© (Am√©lior√© avec Batch Normalization)**
```python
class UnifiedLSTMPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, horizons):
        super().__init__()
        
        # LSTM principal avec dropout
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=2,
            dropout=0.3,
            batch_first=True
        )
        
        # NOUVEAU : Batch normalization apr√®s LSTM
        self.lstm_bn = nn.BatchNorm1d(hidden_size)
        
        # Extracteur de features partag√© avec batch norm
        self.feature_extractor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
        )
        
        # T√™te de sortie s√©par√©e pour chaque horizon avec batch norm
        self.output_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.BatchNorm1d(hidden_size // 2),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(hidden_size // 2, 1)
            ) for _ in horizons
        ])
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_hidden = lstm_out[:, -1, :]
        
        # Appliquer batch normalization √† la sortie LSTM
        last_hidden = self.lstm_bn(last_hidden)
        
        # Extraire les features partag√©es  
        features = self.feature_extractor(last_hidden)
        
        # G√©n√©rer des pr√©dictions pour chaque horizon
        outputs = [head(features) for head in self.output_heads]
        return torch.sigmoid(torch.cat(outputs, dim=1))
```

**3. LSTM Hybride Avanc√© (Multi-Scale + Attention)**
```python
class AdvancedHybridLSTM(nn.Module):
    """
    Traitement multi-√©chelle avec m√©canismes d'attention :
    - Fen√™tres fixes : motifs 15min, 1h, 4h
    - Fen√™tre expanding : historique complet avec attention
    - Cross-attention : Entre √©chelles
    - Batch normalization partout
    """
    
    def __init__(self, input_size, fixed_windows=[15, 60, 240]):
        super().__init__()
        
        # LSTMs de fen√™tres fixes (un par √©chelle)
        self.fixed_lstms = nn.ModuleDict({
            str(window): nn.LSTM(input_size, hidden_size//len(fixed_windows))
            for window in fixed_windows
        })
        
        # LSTM de fen√™tre expanding (capacit√© compl√®te)
        self.expanding_lstm = nn.LSTM(input_size, hidden_size, num_layers=2)
        
        # Self-attention pour fen√™tre expanding
        self.self_attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, batch_first=True
        )
        
        # Cross-attention (requ√™tes expanding, cl√©s/valeurs fixes)
        self.cross_attention = nn.MultiheadAttention(
            hidden_size, num_heads=8, batch_first=True
        )
        
        # Couches de fusion et sortie avec batch norm
        self.fusion_layer = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.BatchNorm1d(hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
```

### 6.2 Am√©liorations d'Entra√Ænement

**Focal Loss pour D√©s√©quilibre L√©ger :**
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        
    def forward(self, inputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss
        return focal_loss.mean()
```

**Early Stopping avec Planification du Taux d'Apprentissage :**
```python
# Am√©liorations de la boucle d'entra√Ænement
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=10, verbose=True
)

early_stopping = EarlyStopping(patience=15, min_delta=1e-4)

for epoch in range(max_epochs):
    # Entra√Ænement + validation
    train_loss = train_epoch(model, train_loader, optimizer, criterion)
    val_loss = validate_epoch(model, val_loader, criterion)
    
    # Planification du taux d'apprentissage
    scheduler.step(val_loss)
    
    # V√©rification early stopping
    if early_stopping(val_loss):
        print(f"Early stopping √† l'√©poque {epoch}")
        break
```

### 6.3 M√©triques d'√âvaluation (Focus Financier)

**M√©triques Primaires :**
```python
# ML/utils/metrics_helpers.py
def financial_classification_metrics(y_true, y_pred, returns, y_prob):
    """M√©triques financi√®res compl√®tes pour pr√©diction directionnelle"""
    
    # M√©triques de classification standard
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0.5
    
    # M√©triques sp√©cifiques financi√®res
    # Strat√©gie : Acheter quand pr√©dit HAUT
    strategy_signals = y_pred == 1
    strategy_returns = returns[strategy_signals] if np.any(strategy_signals) else np.array([])
    
    strategy_metrics = {
        'win_rate': np.mean(strategy_returns > 0) if len(strategy_returns) > 0 else 0,
        'avg_return': np.mean(strategy_returns) if len(strategy_returns) > 0 else 0,
        'sharpe_ratio': np.mean(strategy_returns) / np.std(strategy_returns) if len(strategy_returns) > 0 and np.std(strategy_returns) > 0 else 0,
        'max_drawdown': calculate_max_drawdown(strategy_returns) if len(strategy_returns) > 0 else 0,
        'total_trades': len(strategy_returns),
        'hit_ratio': accuracy,  # Pr√©cision directionnelle
    }
    
    return {**classification_metrics, **strategy_metrics}
```

## 7. Structure du R√©pertoire des R√©sultats

**Structure de sortie organis√©e :**
```
ML/results/
‚îú‚îÄ‚îÄ lightgbm/
‚îÇ   ‚îú‚îÄ‚îÄ metrics_walkforward.json
‚îÇ   ‚îú‚îÄ‚îÄ performance_metrics_walkforward.html
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ lightgbm_medium_term/
‚îÇ   ‚îú‚îÄ‚îÄ metrics_walkforward.json
‚îÇ   ‚îî‚îÄ‚îÄ performance_metrics_walkforward.html
‚îú‚îÄ‚îÄ unified_lstm/
‚îÇ   ‚îú‚îÄ‚îÄ unified_lstm_model_walkforward.pth
‚îÇ   ‚îú‚îÄ‚îÄ metrics_walkforward.json
‚îÇ   ‚îú‚îÄ‚îÄ training_curves_walkforward.html
‚îÇ   ‚îî‚îÄ‚îÄ unified_lstm_metrics_walkforward.html
‚îú‚îÄ‚îÄ advanced_hybrid_lstm/
‚îÇ   ‚îú‚îÄ‚îÄ advanced_hybrid_lstm_model_walkforward.pth
‚îÇ   ‚îú‚îÄ‚îÄ metrics_walkforward.json
‚îÇ   ‚îî‚îÄ‚îÄ training_curves_walkforward.html
‚îú‚îÄ‚îÄ lstm_forecasting/
‚îÇ   ‚îú‚îÄ‚îÄ lstm_model_walkforward.pth
‚îÇ   ‚îú‚îÄ‚îÄ metrics_walkforward.json
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_metrics_walkforward.html
‚îî‚îÄ‚îÄ advanced_hybrid_forecasting/
    ‚îú‚îÄ‚îÄ advanced_hybrid_lstm_forecasting_walkforward.pth
    ‚îú‚îÄ‚îÄ forecasting_metrics_walkforward.json
    ‚îî‚îÄ‚îÄ training_curves_walkforward.html
```

**Convention de Nommage des Fichiers :**
- `*_walkforward.*` : R√©sultats de validation walk-forward
- `*.pth` : Checkpoints de mod√®les PyTorch avec √©tat complet
- `metrics_*.json` : M√©triques structur√©es pour analyse
- `*_curves.html` : Graphiques d'entra√Ænement interactifs
- `performance_*.html` : Graphiques de comparaison de mod√®les

## 8. Ex√©cution du Pipeline (Mis √† Jour)

**Ex√©cution compl√®te du pipeline :**

```bash
# 1. Analyse des Donn√©es et Cat√©gorisation
streamlit run data_analysis/app.py
# ‚Üí Analyser 28 000+ tokens
# ‚Üí G√©n√©rer des cat√©gories mutuellement exclusives
# ‚Üí Exporter vers data/processed/

# 2. Nettoyage des Donn√©es Category-Aware  
python data_cleaning/clean_tokens.py
# ‚Üí Appliquer 4 strat√©gies de nettoyage diff√©rentes
# ‚Üí Supprimer la corruption de donn√©es tout en pr√©servant les extr√™mes
# ‚Üí Suppression anti-leakage de prix constants
# ‚Üí Sortie vers data/cleaned/

# 3. Ing√©nierie de Features ML-Safe
python feature_engineering/advanced_feature_engineering.py
# ‚Üí G√©n√©rer des features rolling/expanding seulement
# ‚Üí Appliquer winsorisation (percentiles 0.5-99.5)
# ‚Üí Analyse de fr√©quence FFT
# ‚Üí Sortie vers data/features/

# 4. Entra√Æner TOUS les Mod√®les avec Validation Walk-Forward

# Mod√®les bas√©s sur arbres (splits globaux)
python ML/directional_models/train_lightgbm_model.py
python ML/directional_models/train_lightgbm_model_medium_term.py

# Mod√®les LSTM (splits par-token)  
python ML/directional_models/train_unified_lstm_model.py
python ML/directional_models/train_advanced_hybrid_lstm.py

# Mod√®les de forecasting
python ML/forecasting_models/train_lstm_model.py
python ML/forecasting_models/train_advanced_hybrid_lstm_forecasting.py

# Mod√®les de base
python ML/baseline_models/train_baseline_regressors.py
python ML/baseline_models/train_logistic_regression_baseline.py

# 5. Analyse et Visualisation
streamlit run feature_engineering/app.py
# ‚Üí Analyse de corr√©lation
# ‚Üí Importance des features
# ‚Üí Comparaison de mod√®les
```

**Analyse de Longueur des Tokens :**
```bash
# V√©rifier les contraintes de donn√©es pour validation walk-forward
python analyze_token_lengths.py
# ‚Üí G√©n√©rer des graphiques de distribution
# ‚Üí Valider la faisabilit√© walk-forward
# ‚Üí Sortie : ML/results/token_length_distribution.png
```

## 9. D√©cisions Techniques Cl√©s et Rationale

### 9.1 Pourquoi des Features Pr√©-Ing√©nieur√©es ?
- **Consistance** : Features identiques √† travers 8 mod√®les diff√©rents
- **Performance** : 10x plus rapide d'it√©ration et d'exp√©rimentation
- **Validation** : D√©tection de leakage et debugging de features plus faciles
- **Modularit√©** : S√©paration propre entre ing√©nierie de features et mod√©lisation
- **Reproductibilit√©** : Processus de g√©n√©ration de features d√©terministe

### 9.2 Pourquoi le Scaling Par-Token ?
- **Variance d'√âchelle** : Les prix des tokens varient de 6+ ordres de grandeur ($0.0000001 √† $100+)
- **Mouvement Relatif** : Pr√©serve les mouvements de pourcentage au sein des tokens
- **√âquit√© du Mod√®le** : Emp√™che les tokens √† gros prix de dominer les gradients
- **Trading R√©aliste** : Imite comment les vrais traders analysent les tokens individuels

### 9.3 Pourquoi la Validation Walk-Forward ?
- **R√©alisme Temporel** : Imite exactement les conditions de trading r√©elles
- **Pas de Leakage Futur** : Math√©matiquement impossible d'utiliser des donn√©es futures
- **Points de Validation Multiples** : 4-7 folds par mod√®le pour des m√©triques robustes
- **Gestion Non-Stationnaire** : Les march√©s crypto changent rapidement, splits fixes irr√©alistes
- **Estimations Conservatrices** : M√©triques de performance plus basses mais plus fiables

### 9.4 Pourquoi l'Approche Bas√©e sur Cat√©gories ?
- **Comportement H√©t√©rog√®ne** : Diff√©rents types de tokens n√©cessitent un traitement diff√©rent
- **Qualit√© des Donn√©es** : Les tokens normaux fournissent des donn√©es d'entra√Ænement de plus haute qualit√©
- **Cycles de Vie Complets** : Les tokens morts montrent des motifs complets de pump-and-dump
- **Dynamiques Extr√™mes** : Les tokens volatils capturent des comportements de march√© uniques
- **Anti-Leakage** : Le nettoyage des tokens morts pr√©vient l'apprentissage trivial

### 9.5 Pourquoi Pr√©server les Mouvements Extr√™mes ?
- **R√©alit√© du March√©** : Les pumps de 10 000% (100x) arrivent en minutes sur les march√©s de memecoins
- **Valeur Pr√©dictive** : Les mouvements extr√™mes sont les plus profitables √† pr√©dire
- **Int√©grit√© des Donn√©es** : Supprimer seulement la corruption √©vidente (>1M%), garder la volatilit√© l√©gitime
- **Robustesse du Mod√®le** : Les mod√®les doivent g√©rer les vraies conditions de march√© crypto

## 10. D√©tails d'Impl√©mentation Critiques

### 10.1 Pr√©vention du Leakage de Donn√©es
```python
# Couches multiples de pr√©vention du leakage :

# 1. Ing√©nierie de features : Seulement features rolling/expanding
features = {
    'rsi_14': calculate_rsi(prices[:current_minute]),  # Pas de donn√©es futures
    'rolling_vol': prices[:current_minute].std(),     # Pas de donn√©es futures
    'expanding_mean': prices[:current_minute].mean()  # Pas de donn√©es futures
}

# 2. Scaling : Ajuster seulement sur le split d'entra√Ænement
scaler.fit(token_data[:train_split_idx])  # Pas de donn√©es validation/test

# 3. Walk-forward : Splits strictement temporels
for fold in walk_forward_folds:
    train_data = data[:fold_split_time]   # Seulement donn√©es pass√©es
    test_data = data[fold_split_time:]    # Seulement donn√©es futures

# 4. Nettoyage tokens morts : Supprimer p√©riodes constantes
if constant_minutes >= 60:
    df = df[:-constant_minutes+2]  # Pr√©venir apprentissage trivial
```

### 10.2 Gestion de la Distribution de Longueur des Tokens
```python
# Distribution actuelle apr√®s nettoyage (de analyze_token_lengths.py) :
# Min : 25 minutes, Max : 1940 minutes
# M√©diane : 1380 minutes (23 heures)
# 75√®me percentile : 1380 minutes

# Walk-forward s'adapte √† la longueur du token :
if token_length < 600:      # Tokens courts
    config = 'short'        # 4h train min, √©tapes 1h
elif token_length < 1500:   # Tokens moyens  
    config = 'medium'       # 8h train min, √©tapes 2h
else:                       # Tokens longs
    config = 'long'         # 16h train min, √©tapes 4h
```

### 10.3 Am√©liorations d'Architecture des Mod√®les

**Avantages de la Batch Normalization :**
- **Stabilit√© d'Entra√Ænement** : R√©duit le covariate shift interne
- **Convergence Plus Rapide** : Taux d'apprentissage plus √©lev√©s possibles
- **R√©gularisation** : R√©duit le surapprentissage
- **Flux de Gradient** : Meilleure propagation de gradient dans les r√©seaux profonds

**Avantages du Traitement Multi-√âchelle :**
- **Reconnaissance de Motifs** : Capture les motifs court-terme (15m), moyen-terme (1h), long-terme (4h)
- **M√©canismes d'Attention** : Se concentre sur les p√©riodes temporelles les plus pertinentes
- **Fusion de Features** : Combine les insights √† travers les √©chelles temporelles
- **Robustesse** : Moins sensible √† une seule √©chelle temporelle

## 11. Performance Attendue et Interpr√©tation

### 11.1 Attentes de Performance R√©alistes

**Comparaison Walk-Forward vs. Split Fixe :**
```
                    Split Fixe    Walk-Forward
Pr√©cision LightGBM:    68-72%        58-65%
Pr√©cision LSTM:        70-75%        60-68%
LSTM Avanc√©:           72-78%        65-72%

Walk-forward donne des m√©triques plus basses mais plus r√©alistes.
```

**Interpr√©tation des M√©triques Financi√®res :**
- **Pr√©cision Directionnelle >55%** : Significativement meilleur que le hasard (50%)
- **Ratio de Sharpe >0.5** : Rendements ajust√©s au risque d√©cents
- **Taux de Gain >52%** : Esp√©rance l√©g√®rement positive
- **Drawdown Max <20%** : Gestion de risque raisonnable

### 11.2 Guides de Comparaison de Mod√®les

**Quand Utiliser Chaque Mod√®le :**
- **LightGBM** : Rapide, interpr√©table, g√®re bien les features cat√©gorielles
- **LSTM Unifi√©** : Bon √©quilibre performance et simplicit√©
- **Hybride Avanc√©** : Meilleure performance, plus complexe, n√©cessite plus de donn√©es
- **Mod√®les de Forecasting** : Quand vous avez besoin de pr√©dictions de prix r√©elles, pas juste la direction

**Approche d'Ensemble :**
```python
# Combiner les pr√©dictions de mod√®les multiples
ensemble_prediction = (
    0.3 * lightgbm_pred + 
    0.3 * unified_lstm_pred + 
    0.4 * advanced_lstm_pred
)
```

## 12. D√©pannage et Probl√®mes Courants

### 12.1 Probl√®mes d'Entra√Ænement
```python
# Datasets vides apr√®s splitting walk-forward
if len(train_dataset) == 0:
    # V√©rifier : Ing√©nierie de features compl√©t√©e ?
    # V√©rifier : Seuil de longueur minimum de token (400 min)
    # V√©rifier : Configuration WalkForwardSplitter

# CUDA out of memory
# R√©duire la taille de batch ou utiliser gradient checkpointing
CONFIG['batch_size'] = 16  # Au lieu de 32

# Losses NaN pendant l'entra√Ænement  
# V√©rifier : Winsorizer appliqu√© aux features ?
# V√©rifier : Features √† variance nulle corrig√©es ?
# V√©rifier : Taux d'apprentissage pas trop √©lev√© ?
```

### 12.2 Probl√®mes de Qualit√© des Donn√©es
```python
# Tokens avec donn√©es insuffisantes
min_token_length = 400  # Augmenter si obtenez des folds vides

# Outliers extr√™mes cassant les mod√®les
# Augmenter winsorisation : lower_quantile=0.001, upper_quantile=0.999

# Features constantes dans tokens morts
# S'assurer que le nettoyage minimal supprime les p√©riodes constantes
```

---

## 13. Le√ßons Critiques Apprises

1. **Le Leakage de Donn√©es est Extr√™mement Subtil** : M√™me des features apparemment innocentes comme "volatilit√© totale" peuvent fuiter de l'information future
2. **L'Ordre Temporel est Sacr√©** : Ne jamais m√©langer les p√©riodes temporelles - les march√©s crypto sont hautement non-stationnaires
3. **L'√âchelle Compte √ânorm√©ment** : 6+ ordres de grandeur de variation de prix n√©cessitent une normalisation soigneuse par-token
4. **Les Tokens Morts sont de l'Or** : Les cycles de vie complets fournissent des donn√©es de motifs pump-and-dump inestimables
5. **Les Mouvements Extr√™mes sont R√©els** : Les pumps 100x en minutes sont un comportement de march√© l√©gitime, pas des erreurs de donn√©es
6. **Walk-Forward est Essentiel** : Les splits fixes donnent des r√©sultats irr√©alistement optimistes dans les march√©s non-stationnaires
7. **Traitement Category-Aware** : Diff√©rents comportements de tokens n√©cessitent diff√©rentes strat√©gies de gestion des donn√©es
8. **La Batch Normalization Transforme l'Entra√Ænement** : Am√©liore dramatiquement la stabilit√© et performance d'entra√Ænement LSTM
9. **Les Motifs Multi-√âchelle Comptent** : Combiner des motifs 15m, 1h, et 4h capture plus de dynamiques de march√©
10. **M√©triques Financi√®res > M√©triques ML** : La pr√©cision est moins importante que les rendements ajust√©s au risque et le drawdown

---

**Statut d'Impl√©mentation Final** : Cette m√©thodologie a √©t√© enti√®rement impl√©ment√©e et valid√©e sur 28 000+ memecoins Solana avec validation walk-forward rigoureuse √† travers 8 architectures de mod√®les diff√©rentes. Tous les mod√®les utilisent maintenant un splitting temporel r√©aliste et atteignent 58-72% de pr√©cision directionnelle avec pr√©vention appropri√©e du leakage.

**Sorties des Mod√®les** : Tous les mod√®les entra√Æn√©s, m√©triques, et visualisations sont sauvegard√©s dans `ML/results/` avec les r√©sultats de validation walk-forward clairement marqu√©s pour identification et comparaison faciles.