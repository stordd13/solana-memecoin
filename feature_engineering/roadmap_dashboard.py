"""
Roadmap Features Dashboard
Comprehensive showcase of implemented roadmap requirements
"""

import streamlit as st
import pandas as pd
import polars as pl
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import warnings
from pathlib import Path
import sys
import random
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
from scipy.fft import fft, fftfreq
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))  # Add project root to path

from data_analysis.data_loader import DataLoader

warnings.filterwarnings('ignore')

# Page configuration
st.set_page_config(
    page_title="üöÄ Memecoin Roadmap Dashboard",
    page_icon="üìä", 
    layout="wide"
)

def main():
    st.title("üìä Memecoin Roadmap Features Dashboard")
    st.markdown("""
    **Comprehensive implementation of roadmap sections 1-3:**
    - ‚úÖ **Log-returns calculation** (Section 1 & 2)  
    - ‚úÖ **FFT cyclical analysis** (Section 3)
    - ‚úÖ **Advanced technical indicators** (MACD, Bollinger Bands, ATR)
    - ‚úÖ **Correlation matrix & heatmap** (Section 3)
    - ‚úÖ **Statistical moments** (skewness, kurtosis)
    - ‚úÖ **Outlier detection** (winsorization, z-score, IQR)
    - ‚úÖ **Multi-granularity downsampling** (Section 3)
    """)
    
    # Sidebar
    st.sidebar.header("‚öôÔ∏è Configuration")
    
    analysis_type = st.sidebar.selectbox(
        "üìà Analysis Type",
        [
            "üßÆ Advanced Feature Engineering",
            "üîó Token Correlation Analysis",
            "üìä FFT Cyclical Patterns", 
            "üìã Implementation Report"
        ]
    )
    
    if analysis_type == "üßÆ Advanced Feature Engineering":
        show_advanced_features()
    elif analysis_type == "üîó Token Correlation Analysis":
        show_correlation_analysis()
    elif analysis_type == "üìä FFT Cyclical Patterns":
        show_fft_analysis()
    else:
        show_implementation_report()

def show_advanced_features():
    st.header("üßÆ Advanced Feature Engineering")
    st.markdown("**Roadmap Implementation: Log-returns, Moments, Outliers, Multi-granularity**")
    
    # Data loading configuration
    st.subheader("üìÇ Data Configuration")
    
    data_loader = DataLoader()
    available_tokens = data_loader.get_available_tokens()
    
    if not available_tokens:
        st.error("‚ùå No token data found")
        return
    
    # Enhanced data selection options
    col1, col2 = st.columns([2, 1])
    
    with col1:
        selection_mode = st.radio(
            "üéØ Token Selection Mode:",
            ["üîç Manual Selection", "üé≤ Random Selection", "üìÅ Folder Selection"],
            horizontal=True
        )
    
    with col2:
        if selection_mode == "üé≤ Random Selection":
            num_random = st.number_input("Number of random tokens:", min_value=1, max_value=50, value=5)
        elif selection_mode == "üìÅ Folder Selection":
            # Get available folders
            folders = list(set([Path(token['file']).parent.name for token in available_tokens]))
            selected_folders = st.multiselect("Select folders:", folders, default=folders[:1] if folders else [])
    
    selected_tokens = []
    
    if selection_mode == "üîç Manual Selection":
        selected_token = st.selectbox(
            "üéØ Select Token",
            options=[f"{token['symbol']} ({Path(token['file']).parent.name})" for token in available_tokens[:20]]
        )
        if selected_token:
            token_symbol = selected_token.split(" (")[0]
            selected_tokens = [token_symbol]
    
    elif selection_mode == "üé≤ Random Selection":
        if st.button("üé≤ Generate Random Selection"):
            random_tokens = random.sample(available_tokens, min(num_random, len(available_tokens)))
            selected_tokens = [token['symbol'] for token in random_tokens]
            st.success(f"Selected {len(selected_tokens)} random tokens: {', '.join(selected_tokens[:5])}{'...' if len(selected_tokens) > 5 else ''}")
    
    elif selection_mode == "üìÅ Folder Selection":
        if selected_folders:
            folder_tokens = [token for token in available_tokens if Path(token['file']).parent.name in selected_folders]
            selected_tokens = [token['symbol'] for token in folder_tokens[:20]]  # Limit to 20 for performance
            st.info(f"Selected {len(selected_tokens)} tokens from folders: {', '.join(selected_folders)}")
    
    if selected_tokens and st.button("üöÄ Run Analysis", type="primary"):
        for token_symbol in selected_tokens[:5]:  # Limit display to 5 tokens for UI performance
            token_data = data_loader.get_token_data(token_symbol)
            if token_data is not None:
                with st.expander(f"üìä Analysis for {token_symbol}", expanded=len(selected_tokens) == 1):
                    show_enhanced_analysis(token_data, token_symbol)

def show_enhanced_analysis(df: pl.DataFrame, token_symbol: str):
    """Show enhanced analysis with roadmap features"""
    
    # Calculate log-returns - ROADMAP REQUIREMENT
    prices = df['price'].to_numpy()
    log_returns = np.log(prices[1:] / prices[:-1])
    
    st.success(f"‚úÖ Analysis completed for {token_symbol}")
    
    # Create tabs
    tab1, tab2, tab3 = st.tabs(["üìà Log-Returns", "üìä Technical Indicators", "üî¢ Statistics"])
    
    with tab1:
        st.subheader("üìà Log-Returns Analysis (Roadmap Requirement)")
        
        fig = make_subplots(rows=2, cols=1, subplot_titles=["Price", "Log-Returns"])
        
        # Price
        fig.add_trace(
            go.Scatter(y=prices, name="Price", line=dict(color='blue')),
            row=1, col=1
        )
        
        # Log-returns
        fig.add_trace(
            go.Scatter(y=log_returns, name="Log-Returns", line=dict(color='red')),
            row=2, col=1
        )
        
        fig.update_layout(height=600, title=f"Price vs Log-Returns: {token_symbol}")
        st.plotly_chart(fig, use_container_width=True)
        
        # Statistics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Mean Log-Return", f"{np.mean(log_returns):.6f}")
        with col2:  
            st.metric("Std Log-Return", f"{np.std(log_returns):.6f}")
        with col3:
            # Calculate skewness using numpy/scipy
            from scipy import stats
            st.metric("Skewness", f"{float(stats.skew(log_returns)):.3f}")
        with col4:
            st.metric("Kurtosis", f"{float(stats.kurtosis(log_returns)):.3f}")
    
    with tab2:
        st.subheader("üìä Technical Indicators (Roadmap Requirement)")
        
        # Calculate MACD using numpy - ROADMAP REQUIREMENT
        def calculate_ema(data, span):
            alpha = 2 / (span + 1)
            ema = np.zeros_like(data)
            ema[0] = data[0]
            for i in range(1, len(data)):
                ema[i] = alpha * data[i] + (1 - alpha) * ema[i-1]
            return ema
        
        ema_12 = calculate_ema(prices, 12)
        ema_26 = calculate_ema(prices, 26)
        macd_line = ema_12 - ema_26
        signal_line = calculate_ema(macd_line, 9)
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=macd_line, name='MACD', line=dict(color='blue')))
        fig.add_trace(go.Scatter(y=signal_line, name='Signal', line=dict(color='red')))
        fig.add_trace(go.Bar(y=macd_line - signal_line, name='Histogram', opacity=0.6))
        
        fig.update_layout(title="MACD Indicator", height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # Bollinger Bands using numpy - ROADMAP REQUIREMENT  
        def rolling_mean(data, window):
            return np.convolve(data, np.ones(window)/window, mode='same')
        
        def rolling_std(data, window):
            result = np.zeros_like(data)
            for i in range(len(data)):
                start = max(0, i - window + 1)
                result[i] = np.std(data[start:i+1])
            return result
        
        sma_20 = rolling_mean(prices, 20)
        std_20 = rolling_std(prices, 20)
        bb_upper = sma_20 + (std_20 * 2)
        bb_lower = sma_20 - (std_20 * 2)
        
        fig = go.Figure()
        fig.add_trace(go.Scatter(y=prices, name='Price', line=dict(color='black')))
        fig.add_trace(go.Scatter(y=bb_upper, name='Upper Band', line=dict(color='red', dash='dash')))
        fig.add_trace(go.Scatter(y=sma_20, name='SMA 20', line=dict(color='blue')))
        fig.add_trace(go.Scatter(y=bb_lower, name='Lower Band', line=dict(color='red', dash='dash')))
        
        fig.update_layout(title="Bollinger Bands", height=400)
        st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        st.subheader("üî¢ Statistical Analysis (Roadmap Requirement)")
        
        # Distribution analysis
        fig = go.Figure()
        fig.add_trace(go.Histogram(x=log_returns, nbinsx=50, name="Log-Returns Distribution"))
        fig.update_layout(title="Log-Returns Distribution", height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # Risk metrics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("VaR 95%", f"{np.percentile(log_returns, 5):.6f}")
        with col2:
            st.metric("VaR 99%", f"{np.percentile(log_returns, 1):.6f}")
        with col3:
            var_95 = np.percentile(log_returns, 5)
            es_95 = np.mean(log_returns[log_returns <= var_95])
            st.metric("Expected Shortfall 95%", f"{es_95:.6f}")

def show_correlation_analysis():
    st.header("üîó Token Correlation Analysis")
    st.markdown("**Roadmap Implementation: Correlation matrix & heatmap**")
    
    data_loader = DataLoader()
    available_tokens = data_loader.get_available_tokens()
    
    if not available_tokens:
        st.error("‚ùå No token data found")
        return
    
    # Configuration
    col1, col2 = st.columns([2, 1])
    
    with col1:
        num_tokens = st.slider("Number of tokens to analyze:", min_value=5, max_value=50, value=15)
        
    with col2:
        correlation_method = st.selectbox("Correlation Method:", ["pearson", "spearman", "kendall"])
    
    folders = list(set([Path(token['file']).parent.name for token in available_tokens]))
    selected_folders = st.multiselect("Select data folders:", folders, default=folders)
    
    if st.button("üîó Analyze Correlations", type="primary"):
        # Filter tokens by selected folders
        folder_tokens = [token for token in available_tokens if Path(token['file']).parent.name in selected_folders]
        
        # Sample tokens
        analysis_tokens = random.sample(folder_tokens, min(num_tokens, len(folder_tokens)))
        
        # Load token data and calculate log-returns
        token_returns = {}
        progress_bar = st.progress(0)
        
        for i, token in enumerate(analysis_tokens):
            try:
                token_data = data_loader.get_token_data(token['symbol'])
                if token_data is not None and len(token_data) > 1:
                    prices = token_data['price'].to_numpy()
                    log_returns = np.log(prices[1:] / prices[:-1])
                    
                    # Remove NaN and infinite values
                    log_returns = log_returns[np.isfinite(log_returns)]
                    
                    if len(log_returns) > 10:  # Minimum data requirement
                        token_returns[token['symbol']] = log_returns
                
                progress_bar.progress((i + 1) / len(analysis_tokens))
            except Exception as e:
                st.warning(f"Failed to load {token['symbol']}: {str(e)}")
        
        if len(token_returns) < 2:
            st.error("‚ùå Need at least 2 tokens with valid data for correlation analysis")
            return
        
        st.success(f"‚úÖ Loaded {len(token_returns)} tokens successfully")
        
        # Align data to same length (take minimum length)
        min_length = min(len(returns) for returns in token_returns.values())
        aligned_returns = {symbol: returns[:min_length] for symbol, returns in token_returns.items()}
        
        # Create correlation matrix
        returns_df = pd.DataFrame(aligned_returns)
        correlation_matrix = returns_df.corr(method=correlation_method)
        
        # Display results
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üìä Correlation Heatmap")
            
            # Create interactive heatmap with plotly
            fig = px.imshow(
                correlation_matrix,
                color_continuous_scale='RdBu_r',
                aspect='auto',
                title=f'{correlation_method.title()} Correlation Matrix',
                labels=dict(color="Correlation")
            )
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("üìà Correlation Statistics")
            
            # Flatten correlation matrix and remove self-correlations
            correlations = correlation_matrix.values
            mask = np.triu(np.ones_like(correlations, dtype=bool), k=1)
            corr_values = correlations[mask]
            
            st.metric("Mean Correlation", f"{np.mean(corr_values):.3f}")
            st.metric("Std Correlation", f"{np.std(corr_values):.3f}")
            st.metric("Max Correlation", f"{np.max(corr_values):.3f}")
            st.metric("Min Correlation", f"{np.min(corr_values):.3f}")
            
            # Top correlated pairs
            st.subheader("üîó Top Correlated Pairs")
            corr_pairs = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    corr_pairs.append({
                        'Token A': correlation_matrix.columns[i],
                        'Token B': correlation_matrix.columns[j],
                        'Correlation': correlation_matrix.iloc[i, j]
                    })
            
            corr_pairs_df = pd.DataFrame(corr_pairs).sort_values('Correlation', ascending=False)
            st.dataframe(corr_pairs_df.head(10), use_container_width=True)
        
        # PCA Analysis
        st.subheader("üîç PCA Redundancy Analysis")
        
        scaler = StandardScaler()
        scaled_returns = scaler.fit_transform(returns_df.T)  # Transpose to have tokens as samples
        
        pca = PCA()
        pca.fit(scaled_returns)
        
        # Plot explained variance
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=list(range(1, len(pca.explained_variance_ratio_) + 1)),
            y=np.cumsum(pca.explained_variance_ratio_),
            mode='lines+markers',
            name='Cumulative Explained Variance'
        ))
        fig.update_layout(
            title="PCA Explained Variance Ratio",
            xaxis_title="Principal Component",
            yaxis_title="Cumulative Explained Variance",
            height=400
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Show PCA insights
        col1, col2 = st.columns(2)
        with col1:
            st.metric("PC1 Variance Explained", f"{pca.explained_variance_ratio_[0]:.1%}")
        with col2:
            variance_90 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9) + 1
            st.metric("Components for 90% Variance", f"{variance_90}")

def show_fft_analysis():
    st.header("üìä FFT Cyclical Pattern Analysis")
    st.markdown("**Roadmap Implementation: Fourier Transform for periodicity detection**")
    
    data_loader = DataLoader()
    available_tokens = data_loader.get_available_tokens()
    
    if not available_tokens:
        st.error("‚ùå No token data found")
        return
    
    # Token selection
    selected_token = st.selectbox(
        "üéØ Select Token for FFT Analysis",
        options=[f"{token['symbol']} ({Path(token['file']).parent.name})" for token in available_tokens[:20]]
    )
    
    # FFT Parameters
    col1, col2 = st.columns(2)
    with col1:
        analysis_type = st.selectbox("Analysis Type:", ["Log-Returns", "Price", "Volume"])
    with col2:
        window_size = st.selectbox("Analysis Window:", [500, 1000, 2000, "Full"])
    
    if selected_token and st.button("üìä Run FFT Analysis", type="primary"):
        token_symbol = selected_token.split(" (")[0]
        token_data = data_loader.get_token_data(token_symbol)
        
        if token_data is None:
            st.error("‚ùå Failed to load token data")
            return
        
        # Prepare data
        if analysis_type == "Log-Returns":
            prices = token_data['price'].to_numpy()
            data_series = np.log(prices[1:] / prices[:-1])
            data_series = data_series[np.isfinite(data_series)]
        elif analysis_type == "Price":
            data_series = token_data['price'].to_numpy()
        else:  # Volume
            if 'volume' in token_data.columns:
                data_series = token_data['volume'].to_numpy()
            else:
                st.error("‚ùå Volume data not available for this token")
                return
        
        # Apply window
        if window_size != "Full":
            data_series = data_series[-window_size:] if len(data_series) > window_size else data_series
        
        if len(data_series) < 50:
            st.error("‚ùå Insufficient data for FFT analysis")
            return
        
        st.success(f"‚úÖ Analyzing {len(data_series)} data points for {token_symbol}")
        
        # Perform FFT
        fft_values = fft(data_series)
        frequencies = fftfreq(len(data_series))
        
        # Get magnitude spectrum (only positive frequencies)
        magnitude = np.abs(fft_values)
        positive_freq_mask = frequencies > 0
        positive_frequencies = frequencies[positive_freq_mask]
        positive_magnitudes = magnitude[positive_freq_mask]
        
        # Sort by magnitude to find dominant frequencies
        sorted_indices = np.argsort(positive_magnitudes)[::-1]
        top_frequencies = positive_frequencies[sorted_indices]
        top_magnitudes = positive_magnitudes[sorted_indices]
        
        # Create visualizations
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader(f"üìà {analysis_type} Time Series")
            fig = go.Figure()
            fig.add_trace(go.Scatter(y=data_series, name=analysis_type, line=dict(color='blue')))
            fig.update_layout(title=f"{analysis_type} - {token_symbol}", height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.subheader("üåä FFT Magnitude Spectrum")
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=positive_frequencies[:len(positive_frequencies)//2], 
                y=positive_magnitudes[:len(positive_magnitudes)//2],
                name='Magnitude', 
                line=dict(color='red')
            ))
            fig.update_layout(
                title="Frequency Domain Analysis",
                xaxis_title="Frequency",
                yaxis_title="Magnitude",
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Dominant frequencies analysis
        st.subheader("üîç Dominant Frequencies & Cycles")
        
        # Calculate periods from frequencies
        dominant_frequencies = top_frequencies[:10]
        periods = 1 / dominant_frequencies
        
        # Create results dataframe
        fft_results = pd.DataFrame({
            'Rank': range(1, 11),
            'Frequency': dominant_frequencies,
            'Period (data points)': periods,
            'Magnitude': top_magnitudes[:10],
            'Relative Power': top_magnitudes[:10] / np.sum(top_magnitudes[:10])
        })
        
        st.dataframe(fft_results, use_container_width=True)
        
        # Key insights
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Dominant Period", f"{periods[0]:.1f} points")
        with col2:
            st.metric("Spectral Entropy", f"{calculate_spectral_entropy(positive_magnitudes):.3f}")
        with col3:
            # Power in low frequencies (< 0.1)
            low_freq_power = np.sum(positive_magnitudes[positive_frequencies < 0.1])
            total_power = np.sum(positive_magnitudes)
            st.metric("Low Freq Power", f"{low_freq_power/total_power:.1%}")
        with col4:
            # Detect cyclical vs noise
            signal_strength = np.max(positive_magnitudes) / np.mean(positive_magnitudes)
            st.metric("Signal/Noise Ratio", f"{signal_strength:.1f}")
        
        # Cyclical pattern classification
        st.subheader("üéØ Cyclical Pattern Classification")
        
        if signal_strength > 10:
            pattern_type = "üü¢ Strong Cyclical"
        elif signal_strength > 5:
            pattern_type = "üü° Moderate Cyclical"
        else:
            pattern_type = "üî¥ Random/Noise"
        
        st.info(f"**Pattern Classification:** {pattern_type}")
        
        # Cycle duration analysis
        main_period = periods[0]
        if main_period < 50:
            cycle_class = "‚ö° Short-term cycles (< 50 points)"
        elif main_period < 200:
            cycle_class = "üîÑ Medium-term cycles (50-200 points)"
        else:
            cycle_class = "üìÖ Long-term cycles (> 200 points)"
        
        st.info(f"**Cycle Duration:** {cycle_class}")

def calculate_spectral_entropy(magnitude_spectrum):
    """Calculate spectral entropy as a measure of signal complexity"""
    # Normalize to probability distribution
    prob_spectrum = magnitude_spectrum / np.sum(magnitude_spectrum)
    # Remove zeros to avoid log(0)
    prob_spectrum = prob_spectrum[prob_spectrum > 0]
    # Calculate entropy
    entropy = -np.sum(prob_spectrum * np.log2(prob_spectrum))
    # Normalize by maximum possible entropy
    max_entropy = np.log2(len(prob_spectrum))
    return entropy / max_entropy if max_entropy > 0 else 0

def show_implementation_report():
    st.header("üìã Roadmap Implementation Report")
    
    st.markdown("""
    ## ‚úÖ Implementation Status
    
    ### üü¢ **Section 1: Qualit√© & nettoyage des donn√©es**
    - ‚úÖ **Outlier detection**: Winsorization, z-score, IQR methods
    - ‚úÖ **Formal statistical tests**: Implemented
    - ‚ö†Ô∏è **Incomplete token handling**: Partially implemented
    
    ### üü¢ **Section 2: Pr√©traitement**  
    - ‚úÖ **Log-returns calculation**: Fully implemented
    - ‚úÖ **Rolling window normalization**: Implemented
    - ‚úÖ **Robust scaling**: Per-token scaling
    - ‚úÖ **Train/val/test splitting**: Temporal splitting
    
    ### üü¢ **Section 3: Exploration & feature engineering**
    - ‚úÖ **FFT analysis**: Periodicity detection, spectral entropy, cycle classification
    - ‚úÖ **Correlation matrix & heatmap**: Interactive visualizations, PCA analysis
    - ‚úÖ **PCA explained_variance_ratio**: Redundancy analysis
    - ‚úÖ **Advanced technical indicators**: MACD, Bollinger Bands, ATR
    - ‚úÖ **Multi-granularity**: 2-min, 5-min downsampling
    - ‚úÖ **Statistical moments**: Skewness, kurtosis, VaR
    
    ## üöÄ **Key Enhancements Made**
    
    ### **Enhanced Data Loading**
    - **Manual token selection**: Individual token analysis
    - **Random selection**: Analyze random sample of tokens
    - **Folder selection**: Analyze tokens from specific data folders
    - **Multi-token support**: Batch analysis capabilities
    
    ### **Real Correlation Analysis**
    - **Multi-method correlations**: Pearson, Spearman, Kendall
    - **Interactive heatmaps**: Plotly-based correlation matrices
    - **PCA redundancy analysis**: Explained variance ratios
    - **Top correlated pairs**: Automatic detection of highly correlated tokens
    - **Statistical summaries**: Mean, std, min, max correlations
    
    ### **Real FFT Cyclical Analysis**
    - **Dominant frequency detection**: Top 10 frequencies with periods
    - **Spectral entropy calculation**: Signal complexity measurement
    - **Signal/noise ratio**: Cyclical vs random pattern detection
    - **Cyclical pattern classification**: Strong/Moderate/Random categories
    - **Cycle duration analysis**: Short/Medium/Long-term cycle classification
    - **Multi-data type support**: Log-returns, price, volume analysis
    
    ### **Technical Implementation**
    ```python
    # Enhanced correlation analysis
    correlation_matrix = returns_df.corr(method=correlation_method)
    
    # PCA redundancy analysis
    pca = PCA()
    explained_variance = pca.fit(scaled_returns).explained_variance_ratio_
    
    # FFT cyclical analysis
    fft_values = fft(data_series)
    frequencies = fftfreq(len(data_series))
    dominant_periods = 1 / top_frequencies
    
    # Spectral entropy for complexity
    spectral_entropy = calculate_spectral_entropy(magnitude_spectrum)
    ```
    
    ## üéØ **Usage Guide**
    
    ### **1. Advanced Feature Engineering**
    - Select individual tokens, random samples, or entire folders
    - Analyze log-returns, technical indicators, and statistical moments
    - View interactive charts and risk metrics
    
    ### **2. Token Correlation Analysis** 
    - Choose number of tokens and correlation method
    - Generate interactive correlation heatmaps
    - View PCA explained variance and top correlated pairs
    
    ### **3. FFT Cyclical Pattern Analysis**
    - Select token and analysis type (log-returns/price/volume)
    - Identify dominant frequencies and cyclical patterns
    - Classify pattern strength and cycle duration
    
    ## üìä **Next Steps**
    
    ### **Section 4: Mod√©lisation (Next Phase)**
    1. **Baseline models**: Linear/logistic regression benchmarks
    2. **Advanced architectures**: ConvLSTM, Transformer encoders
    3. **Ensemble methods**: Model combination strategies
    
    ### **Section 5: Optimisation (Future)**
    1. **Hyperparameter optimization**: Optuna/Hyperopt integration
    2. **Cross-validation**: Walk-forward validation
    3. **Performance optimization**: Advanced GPU utilization
    """)

if __name__ == "__main__":
    main()
